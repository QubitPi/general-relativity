\section{Linear Algebra}

\begin{Definition}{Vector Space \phantomsection\hypertarget{vector-space}}{vector-space}
    The space $\mathbb{R}^n$ consists of all column vectors $\boldsymbol{v}$ with $n$ components\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 120}
\end{Definition}

The components of $\boldsymbol{v}$ are real numbers, which is the reason for the letter $\mathbb{R}$. A vector whose $n$
components are complex numbers lies in the space $\mathbb{C}^n$

\begin{Definition}{Subspace}{subspace}
    A \textbf{subspace}\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 122}
    of a \hyperlink{vector-space}{vector space} is a set of vectors (including $\boldsymbol{0}$) that satisfies 2
    requirements: If $\boldsymbol{v}$ and $\boldsymbol{w}$ are vectors in the subspace and $c$ any scalar, then

    \begin{itemize}
        \item $\boldsymbol{v} + \boldsymbol{w}$ is in the subspace
        \item $c\boldsymbol{v}$ is in the subspace
    \end{itemize}

    In other words, the set of vectors is ``\textbf{closed}" under addition and multiplication - \textbf{all linear combinations
    stay in the subspace}

\end{Definition}

Intuitively, we can visualize a subspace in the 3-dimensional space $\mathbb{R}^3$. Choose a plane through the
origin $(0, 0, 0)$. That plane is a vector space in its own right. If we add two vectors in the plane, their sum is in
the plane; if we multiply an in-plane vector by $2$ or $-5$, it is still in the plane. This plane is a vector space
\textbf{inside $\mathbb{R}^3$} or is a subspace of the full vector space $\mathbb{R}^3$

\begin{Definition}{Column Space \phantomsection\hypertarget{column-space}}{column-space}
    The \textbf{column space}, $C(A)$, consists of all linear combinations of the columns, i.e. the combinations of all
    possible vectors $A\boldsymbol{x}$

    The subspece $C(A)$ is the ``\textbf{span}" of matrix $A$
\end{Definition}

\begin{Definition}{Span \phantomsection\hypertarget{span}}{span}
    A set of vectors spans a space if their linear combinations fill the space
\end{Definition}

\begin{Definition}{Basis for a Vector Space}{basis}
    A \textbf{basis}\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 172} for a vector space is a sequence of vectors with 2 properties

    \begin{itemize}
        \item The basis vectors are linearly independent, and
        \item they \hyperlink{span}{span} the space
    \end{itemize}
\end{Definition}

\subsection{Eigenvalues and Eigenvectors}

\begin{tcolorbox}[
    enhanced,arc=3mm,boxrule=1.5mm,
    frame hidden,colback=blue!10!white,
    borderline={1mm}{0mm}{blue,dotted}
]
    \centering
    All matrices in this section are square
\end{tcolorbox}

Almost all vectors change direction, when they are multiplied by a matrix $A$. Certain exceptional vectors
$\boldsymbol{x}$ don't and are in the same direction as $A\boldsymbol{x}$. Those are the \textbf{eigenvectors}\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 283}. By definition, the result of this multiplication shall be a number times
the original $\boldsymbol{x}$, i.e.

\begin{equation}
    A\boldsymbol{x} = \lambda\boldsymbol{x}
\end{equation}

We call $\lambda$ an \textbf{eigenvalue} of $A$.

The eigenvalue $\lambda$ tells whether the special vector $\boldsymbol{x}$ is stretched (e.g. 2) or shrunk (e.g.
$\frac{1}{2}$) or reversed (-1) or left unchanged (1) when multiplied by $A$. When $\lambda = 0$, Then
$A\boldsymbol{x} = 0\boldsymbol{x}$ means this eigenvector $\boldsymbol{x}$ is in nullspace\footnote{
\begin{Definition}{Nullspace}{nullspace}
    The nullspace of $A$ consists of all solutions to $A\boldsymbol{x} = \boldsymbol{0}$. These vectors $\boldsymbol{x}$
    are in $\mathbb{R}^n$. The nullspace containing all solutions of $A\boldsymbol{x} = \boldsymbol{0}$ is denoted by
    $N(A)$ (\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 132)
\end{Definition}
}

\subsubsection{Matrix Diagonalization}

\begin{tcolorbox}[
    colbacktitle=red!10!white,
    colback=blue!10!white,coltitle=red!70!black,
    title=Why do we study disgonalizing a matrix?
]
    When $\boldsymbol{x}$ is an eigenvector, multiplication by $A$ is just multipliation by a number
    $\lambda$: $A\boldsymbol{x} = \lambda\boldsymbol{x}$. All the difficulties of matrices are swept away, because the
    matrix $A$ turns into a diagonal matrix $\Lambda$ when we use the eigenvectors properly.
\end{tcolorbox}

Suppose a $n$ by $n$ matrix A has $n$ linearly independent eigenvectors $\boldsymbol{x_1}, \ldots, \boldsymbol{x_n}$.
Put them into the columns of an \textbf{eigenvector matrix $S$}. Multiplying $A$ with $S$\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 298}:

\begin{align}
    AS &= A\begin{bmatrix}
              &                &        &                  \\
              \boldsymbol{x_1} & \ldots & \boldsymbol{x_n} \\
              &                &        &                  \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
           &                          &        &                            \\
           \boldsymbol{\lambda_1 x_1} & \ldots & \boldsymbol{\lambda_n x_n} \\
           &                          &        &                            \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
           &                &        &                  \\
           \boldsymbol{x_1} & \ldots & \boldsymbol{x_n} \\
           &                &        &                  \\
    \end{bmatrix}\begin{bmatrix}
                     \lambda_1 &      &         \\
                     &         \ddots &         \\
                     &         &      \lambda_n \\
    \end{bmatrix} \\
    &= S\begin{bmatrix}
            \lambda_1 &      &         \\
            &         \ddots &         \\
            &         &      \lambda_n \\
    \end{bmatrix}
\end{align}

Let

\begin{equation}
    \Lambda = \begin{bmatrix}
                  \lambda_1 &      & \\
                  &         \ddots & \\
                  &         &      \lambda_n
    \end{bmatrix}
\end{equation}

Then

\begin{equation}
    AS = S\Lambda \text{\ \ \ \ \ \ or \ \ \ \ \ \ } S^{-1}AS = \Lambda \text{\ \ \ \ \ \ or \ \ \ \ \ \ } A = S\Lambda S^{-1}
\end{equation}

\includegraphics[width=0.05\textwidth]{可莉-86.png} We call $\Lambda$ the \textbf{Eigenvalue Matrix}

\begin{Definition}{Diagonalization}{diagonalization}
    Suppose a $n$ by $n$ matrix A has $n$ linearly independent eigenvectors
    $\boldsymbol{x_1}, \ldots, \boldsymbol{x_n}$. . Then
    $S^{-1}AS$ is the \textbf{eigenvalue matrix $\Lambda$}

    \begin{equation}
        S^{-1}AS = \Lambda = \begin{bmatrix}
                                \lambda_1 &      & \\
                                &         \ddots & \\
                                &         &      \lambda_n
        \end{bmatrix}
    \end{equation}
\end{Definition}

\begin{Definition}{Bilinear Form \phantomsection\hypertarget{bilinear-form}}{bilinear-form}
    Let $F$ be a field and $V$ a vector space over $F$. A \textit{bilinear form} on $V$ is a function
    $B: V \times V \rightarrow F$ that is linear in each variable when the other one is fixed. That is

    \begin{align}
        B(v + v', w) &= B(v, w) + B(v', w) \\
        B(cv, w)     &= cB(v, w)
    \end{align}

    for all $v, v', w \in V$ and $c \in F$, and

    \begin{align}
        B(v, w + w') &= B(v, w) + B(v, w') \\
        B(v, cw)     &= cB(v, w)
    \end{align}

    for all $v, v, w' \in V$ and $c \in F$

    We call $B$ \textit{symmetric} when

    \begin{equation}
        B(v, w) = B(w, v)
    \end{equation}

    for all $v, w \in V$\footnote{\href{https://kconrad.math.uconn.edu/blurbs/linmultialg/bilinearform.pdf}{Keith Conrad}}

    \begin{tcbraster}[
        raster columns=2,
        raster equal height
    ]
        \begin{Definition}{Linear Transformation}{linear-transformation}
            The transformation is \textbf{linear} if it meets these requirements for all $\boldsymbol{v}$ and $\boldsymbol{w}$\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 375}:

            \begin{equation}
                T(c\boldsymbol{v} + d\boldsymbol{w}) = cT(\boldsymbol{v}) + dT(\boldsymbol{w})
            \end{equation}

            for all $c$ and $d$
        \end{Definition}
        \begin{Definition}{Field}{field}
            A field is a set of elements in which a pair of operations called multiplication and addition is defined analogous
            to the operations of multiplication and addition in the real number system (which is itself an example of a
            field)\footnote{\href{https://projecteuclid.org/ebooks/notre-dame-mathematical-lectures/Chapter-I-Linear-Algebra/chapter/Chapter-I-Linear-Algebra/ndml/1175197044}{A. Fields, Chapter I: Linear Algebra}, Galois Theory: Lectures Delivered at the University of Notre Dame, Project Euclid}
        \end{Definition}
    \end{tcbraster}

    \begin{Definition}{Group}{group}
        A group\footnote{\href{https://www.maths.gla.ac.uk/~mwemyss/teaching/3alg1-7.pdf}{Introduction to Group Theory, Michael Wemyss}} is a non-empty set $G$ together with a rule that assigns to each pair $g$ and $h$ of elements of $G$
        and element $g * h$ such that
        \begin{itemize}
            \item[] \includegraphics[width=0.05\textwidth]{嘟嘟可.png}  $g * h \in G$, which we say $G$ is \textbf{closed} under $*$
            \item[] \includegraphics[width=0.05\textwidth]{嘟嘟可.png} $g * (h * k) = (g * h) * k$ for all $g, h, k \in G$, which we call $*$ being
            \textbf{associative}
            \item[] \includegraphics[width=0.05\textwidth]{嘟嘟可.png} There exists an \textbf{identity} element $e \in G$ such that $e * g = g * e$ for all
            $g \in G$
            \item[] \includegraphics[width=0.05\textwidth]{嘟嘟可.png} Every element $g \in G$ has an \textbf{inverse} $g^{-1}$ such that
            $g * g^{-1} = g^{-1} * g = e$
        \end{itemize}
    \end{Definition}

    \begin{Definition}{Vector Space (Field Theory)}{vector-space-field}
        If $V$ is an additive abelian group with elements $A, B, \ldots$, $F$ a field with elements $a, b, \dots$, and if
        for each $a \in F$ and $A \in V$ the product $aA$ denotes element of $V$, then $V$ is called a
        \textbf{(left) vector space over F} if the following assumptions hold:

        \begin{align}
            a(A + B) &= aA + aB \\
            (a + b)A &= aA + bA \\
            a(bA) &= (ab)A \\
            1A &= A \\
        \end{align}
    \end{Definition}
\end{Definition}