\section{Fundamental principles of special relativity theory (SR)}

\subsection{On ``Principle of relativity (Galileo)"}

\subsubsection{Galilean invariance}

\href{https://en.wikipedia.org/wiki/Newton\%27s_laws_of_motion}{Newton's laws of motion} hold in all frames related
to one another by a \href{https://en.wikipedia.org/wiki/Galilean\_transformation}{Galilean transformation}. In
other words, all frames related to one another by such a transformation are inertial (meaning, Newton's equation of
motion is valid in these frames).\footnote{\href{https://en.wikipedia.org/wiki/Galilean_invariance}{Galilean invariance}}
The proof has been given by the book on page 2.

\section{Construction of the coordinates used by another observer}

\subsection{Why would the tangent of the angle is the speed in Fig. 1.2?}

Suppose $\mathcal{O}$ and $\mathcal{\bar{O}}$ both start out at the same position where $\mathcal{\bar{O}}$
moves along the $x$ at some speed. After $t_1$, observer $\mathcal{O}$ sees $\mathcal{\bar{O}}$ at position $x_1$:

\[ \mathcal{\bar{O}}_1 = (x_1, t_1) \]

Observer  $\mathcal{\bar{O}}$, however, still sees themself at $x = 0$:

\[ \mathcal{\bar{O}}_1 = (0, t_1) \]

By definition where ``$\bar{t}$ is the locus of events at constant $\bar{x} = 0$", $\bar{t}$ is the straight line
that passes the origin and the $(x_1, t_1)$:

\begin{tikzpicture}[scale=1.5]
    \draw[->] (-3,0) node[left] (w) {}--(5,0) node[right] (x) {x};
    \draw[->] (0,-3) node[below] (s) {}--(0,5) node[above] (t) {t};
    \draw[line width=.5pt] (.25,-.25) rectangle (-.25,.25) node (o) {};
    \draw[line width=.5] (o)--(135:1) node[above] () {$\mathcal{\bar{O}}_0$, $\mathcal{O}_0$};

    \draw[black,line width=2pt,->] (0,0)--(4,4) node[above] (t-bar) {{$\bar{t}$}};

    \draw[dashed,line width=.5] (3,0) node[below] (x1) {$x_1$} --(3,3);
    \draw[dashed,line width=.5] (0,3) node[left] (t1) {$t_1$} --(3,3);
    \draw[line width=.5pt] (2.75,3.25) rectangle (3.25,2.75) node (f) {};
    \draw (f) node[right] {$(x_1, t_1)$};

    \draw [blue,thick,domain=225:270] plot ({3 + cos(\x)}, {3 + sin(\x)}) node (speed) {};
    \draw (2.5, 1.8) node[right] (speed) {Tangent of this angle is $\bar{v}$};
\end{tikzpicture}

\section{Invariance of the interval}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    title={Why $(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2 - (\Delta t)^2 = 0$ for two events in the same light beam?}
    \phantomsection\hypertarget{zero-interval}
]

    Let's say, in a simplified 1D case, event $\mathcal{E} = (x_0, t_0)$ and $\mathcal{P} = (x_1, t_1)$.

    \[ (\Delta x)^2 - (\Delta t)^2 = (x_1 - x_0)^2 - (t_1 - t_0)^2 \]

    Since the speed of light is 1,

    \[ (x_1 - x_0)^2 - (t_1 - t_0)^2 = (x_1 - x_0)^2 - (t_1 \times 1 - t_0 \times 1)^2 = (x_1 - x_0)^2 - (x_1 - x_0)^2 = 0 \]
\end{tcolorbox}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    enhanced,
    attach boxed title to top center={yshift=-2mm},
    title={
        \parbox{10cm}{
            Why does the equation contains only $\boldsymbol{M}_{\alpha\beta} + \boldsymbol{M}_{\beta\alpha}$ terms when $\alpha \ne \beta$, which guarantees $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$?
        }
    }]

    \[
        \Delta\bar{s}^2 = \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right)
    \]
\end{tcolorbox}

Before spending too much time on expanding the equation, we can pick up a pair of indices of
$(\alpha, \beta) = (\alpha^*, \beta^*)$ where $\alpha^* \ne \beta^*$. Then we would definitely have the following 2
terms in the expansion:

\[ \boldsymbol{M}_{\alpha^*\beta^*}\left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right) \]
\[ \boldsymbol{M}_{\beta^*\alpha^*}\left(\Delta x^{\beta^*}\right)\left(\Delta x^{\alpha^*}\right) \]

Since

\[ \left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right) = \left(\Delta x^{\beta^*}\right)\left(\Delta x^{\alpha^*}\right) \]

We can then group these 2 terms and factor out the product, leaving

\[
    \left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right)\left( \boldsymbol{M}_{\alpha^*\beta^*} + \boldsymbol{M}_{\beta^*\alpha^*} \right)
\]

The terms of expanded $\Delta\bar{s}^2$ can be expressed in a matrix of

\[
    \renewcommand{\arraystretch}{4}
    \begin{bmatrix}
        \boldsymbol{M}_{00}\Delta x^0\Delta x^0 & \boldsymbol{M}_{01}\Delta x^0\Delta x^1 & \boldsymbol{M}_{02}\Delta x^0\Delta x^2  & \boldsymbol{M}_{03}\Delta x^0\Delta x^3 \\
        \boldsymbol{M}_{10}\Delta x^1\Delta x^0 & \boldsymbol{M}_{11}\Delta x^1\Delta x^1 & \boldsymbol{M}_{12}\Delta x^1\Delta x^2  & \boldsymbol{M}_{13}\Delta x^1\Delta x^3 \\
        \boldsymbol{M}_{20}\Delta x^2\Delta x^0 & \boldsymbol{M}_{21}\Delta x^2\Delta x^1 & \boldsymbol{M}_{22}\Delta x^2\Delta x^2  & \boldsymbol{M}_{23}\Delta x^2\Delta x^3 \\
        \boldsymbol{M}_{30}\Delta x^3\Delta x^0 & \boldsymbol{M}_{31}\Delta x^3\Delta x^1 & \boldsymbol{M}_{32}\Delta x^3\Delta x^2  & \boldsymbol{M}_{33}\Delta x^3\Delta x^3 \\
    \end{bmatrix}
\]

Because the off-diagonal terms always appear in pairs above, we could effectively replace them with their mean value:

\[
    \boldsymbol{M}_{\alpha^*\beta^*} = \boldsymbol{M}_{\beta^*\alpha^*} = \frac{\left( \boldsymbol{M}_{\alpha^*\beta^*} + \boldsymbol{M}_{\beta^*\alpha^*}\right)}{2}
\]

where $\alpha^* \ne \beta^*$. And since $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$ if $\alpha = \beta$, we conclude that

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black]
    \begin{center}
        $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$ for all $\alpha$ and $\beta$
    \end{center}
\end{tcolorbox}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    enhanced,
    attach boxed title to top center={yshift=-2mm},
    title={
        \parbox{10cm}{
            Why do we have the 2nd term in equation
        }
    }]

    \[
        \Delta\bar{s}^2 = \boldsymbol{M}_{00}\left(\Delta r\right)^2 + \tcbhighmath{2\left( \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta x^i \right)\Delta r} + \sum_{i = 1}^3\sum_{i = 1}^3 \boldsymbol{M}_{ij}\Delta x^i\Delta x^j
    \]
\end{tcolorbox}

\begin{align}
    \Delta\bar{s}^2 &= \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \sum_{\alpha = 0}^0\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) + \sum_{\alpha = 0}^3\sum_{\beta = 0}^0 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \sum_{\beta = 0}^3 \boldsymbol{M}_{0\beta}\Delta t \left(\Delta x^{\beta}\right) + \sum_{\alpha = 0}^3 \boldsymbol{M}_{\alpha0}\left(\Delta x^{\alpha}\right)\Delta t + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \boldsymbol{M}_{00}\left( \Delta t \right)^2 + \sum_{\beta = 1}^3 \boldsymbol{M}_{0\beta}\Delta t \left(\Delta x^{\beta}\right) + \sum_{\alpha = 1}^3 \boldsymbol{M}_{\alpha0}\left(\Delta x^{\alpha}\right)\Delta t + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \boldsymbol{M}_{00}\left( \Delta t \right)^2 + \boxed{2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x^i\right) \right]} + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \label{eq:expand}
\end{align}

\begin{tcolorbox}[
    breakable,
    parbox=false,
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    title={Why would $\boldsymbol{M}_{0i} = 0$ for $i = 1, 2, 3$ and $\boldsymbol{M}_{ij} = -\boldsymbol{M}_{00}\delta_{ij}$ in Equation \ref{eq:expand}?}]

    \begin{tcolorbox}[
        colframe=red,
        colback=red!10,
        coltitle=black,
        colbacktitle=red!20,
        title=The answer is: \textbf{not necessarily}. We are probably looking at a wrong problem.
        \phantomsection\hypertarget{wrong-postulate}
    ]
        The solution to exercise 1.8 takes $\Delta x_1 = -\Delta x_2$ to simplify the equation \ref{eq:details}. This is not sufficient, because what if $\Delta x_1 \ne -\Delta x_2$? This box takes a general approach where we \textbf{do not assume any relationship between $\Delta x_1$ and $\Delta x_2$}
    \end{tcolorbox}

    Note that this statement is based on the aforementioned assumption that $\Delta \bar{s}^2 = \Delta s^2 = 0$, which has been proved \hyperlink{zero-interval}{here}. Therefore, by \ref{eq:expand}, we have

    \begin{align}
        &\Delta\bar{s}^2(\Delta t, \Delta x_1) - \Delta\bar{s}^2(\Delta t, \Delta x_2) \\
        ={}& \boldsymbol{M}_{00}\left( \Delta t \right)^2 + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x^i\right) \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
        ={}& 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i\right) \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \nonumber\\
        & 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_2^i\right) \right] - \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) \\
        ={}& \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) + \nonumber\\
        & 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i\right) \right] - 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_2^i\right) \right] \\
        ={}& \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta} \left[ \left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) \right] + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i - \Delta x_2^i\right) \right] = 0 \label{eq:details}
    \end{align}

    We won't be able to go further unless with some assumed relationships between $\Delta x_1^i$ and $\Delta x_2^i$.
    But since \hyperlink{wrong-postulate}{we do not assume any relations between them}\ldots \includegraphics[width=0.1\textwidth]{纳西妲-22.png}

    \ldots let's step back and re-think about this problem then and forget about $\Delta x_1^i$ and $\Delta x_2^i$.

    We go through all these for the proof of invariance of the interval. This is to work out a relation between $\Delta s^2$ and $\Delta \bar{s}^2$. The \roundinlinebox[red]{detail} is about $\Delta x_1^i$ and $\Delta x_2^i$ but the \roundinlinebox[green]{goal} is to derive some form of

    \[
        \Delta \bar{s}^2 = f\left( \Delta s^2 \right) = \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right)
    \]

    where \[ \Delta s^2 = -(\Delta t)^2 + \sum_{i = 1}^3 (\Delta x^i)^2 \]

    \begin{center}
        \textit{Let's work on $f\left( \Delta s^2 \right)$ directly toward that goal then}
    \end{center}
\end{tcolorbox}

Assuming $\Delta \bar{s}^2 = \Delta s^2 = 0$, we have $\Delta t = \pm\Delta x$; plugging it into Eq. \ref{eq:expand} gives us

\begin{align}
    \Delta\bar{s}^2 &= \boldsymbol{M}_{00}\sum_{i = 1}^3\left( \Delta x^i \right)^2 + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\left( \Delta x^i \right)^2 \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \left( \boldsymbol{M}_{00} + 2\boldsymbol{M}_{0i} \right)\sum_{i = 1}^3\left( \Delta x^i \right)^2 + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \label{eq:suggestive}
\end{align}

\textit{Eq.\ref{eq:suggestive} seems to suggest a linear relationship between $\Delta s^2$ and $\Delta\bar{s}^2$}. How do we go about proving it? We now start the \href{https://en.wikipedia.org/wiki/Derivations\_of\_the\_Lorentz\_transformations#Rigorous\_Statement\_and\_Proof\_of\_Proportionality_of_ds2_and_ds\%E2\%80\%B22}{formal proof of \textbf{Invariance of Interval}}

\begin{Theorem}{\href{https://en.wikipedia.org/wiki/Derivations\_of\_the\_Lorentz\_transformations\#Rigorous\_Statement\_and\_Proof\_of\_Proportionality\_of\_ds2\_and\_ds\%E2\%80\%B22}{Proportionality of $ds^2$ and $ds'^2$}}{quadratic-porportionality}
    Let $n, p \ge 1$ be integers, $d := n + p$ and $V$ a vector space over $\mathbb{R}$ of dimension $d$. Let $h$ be
    an \hyperref[def:indefinite-inner-product]{indefinite-inner product} on $V$ with signature type $(n, p)$. Suppose $g$ is a
    \hyperlink{bilinear-form}{symmetric bilinear form} on $V$ such that the null set of the associated quadratic form of
    $h$ is contained in that of $g$ (i.e. suppose that for every $v \in V$, if $h(v, v) = 0$ then $g(v, v) = 0$). Then,
    there exists a constant $C \in \mathbb{R}$ such that $g = Ch$. Futhermore, if we assume $n \ne p$ and that $g$ also
    has signature type $(n, p)$, then we have $C > 0$
\end{Theorem}

\todo[inline]{Prove the theorem}

The assumptions above on $h$ means that $h: V \times V \rightarrow \mathbb{R}$ is a bilinear form which is symmetric
and non-degenerate such that there exists an ordered basis
$\{\boldsymbol{v_1}, \dots, \boldsymbol{v_n}, \boldsymbol{v_{n + 1}}, \dots, \boldsymbol{v_d}\}$ of $V$ for which

\begin{equation}
    h(\boldsymbol{v}_a, \boldsymbol{v}_b) = \begin{cases}
        -1 & a = b \text{, where } a, b \in \{1, \dots, n\}     \\
        1  & a = b \text{, where } a, b \in \{n + 1, \dots, d\} \\
        0  & \text{otherwise}
    \end{cases}
\end{equation}

An equivalent way of saying this is that $h$ has the matrix representation
$\begin{bmatrix}
    -I_n & 0 \\
    0 & I_p \\
\end{bmatrix}$ relative to the ordered basis $\{\boldsymbol{v_1}, \dots, \boldsymbol{v_d}\}$ of $V$

If we are considering the special case where $n = 1, p = 3$ then we are talking about the situation of a matrix
signature in 4-dimensions
