\section{Einstein's Original Paper on ``Special Relativity"}

\begin{marker}
    This section is not from the book, but simply my extra interests on the history of SR
\end{marker}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!85!black
]
    \phantomsection\hypertarget{sr-original-paper}
    The original paper is in \href{https://trello.com/c/ll9Yzd8h}{The Collected Papers of Albert Einstein, Vol.2}, page 140, \textit{On the Electrodynamics of Moving Bodies}
\end{tcolorbox}

Reading the original paper requires the prerequisites of

\begin{itemize}
    \item \href{https://en.wikipedia.org/wiki/Michelson\%E2\%80\%93Morley\_experiment}{Michelson-Morley Experiment}

          \begin{itemize}
              \item \href{https://youtu.be/3G_Q6AggQF8?si=ihpGM23NGq2PfsE2}{An excellent experiment intro}
              \item \highlight[green]{What I care most about this experiment is the \textbf{way we handle "unsolvable"
                    problems}. Michelson-Morley experiment had led to extensive followups trying to explain what was seen in the experiment. All the \textit{mediocre} conclusion simply said: "Dude, we don't know." Albert Einstein innovated a new era of Physics out of this conflict. \textbf{When a problem seems to lead to a dead end, it's time to innovate; it's time to take on the risk and bring the human into a new world of new opportunities!}}
          \end{itemize}

    \item \href{https://trello.com/c/SlPIXwCY}{Maxwell's Electrodynamics}

          \begin{tcolorbox}[
              colbacktitle=red!10!white,
              colback=blue!10!white,coltitle=red!70!black,
              title=Does the Electromagnetic Field \textit{physically} exist?
          ]
              ``There exists a model of the universe which includes a field known as the Electromagnetic Field. This
              model does a remarkably good job of predicting the observations we make in the world. It does so good at
              making such predictions that it is often phrased as 'existing in the
              world"\footnote{\href{https://philosophy.stackexchange.com/a/28010}{https://philosophy.stackexchange.com/a/28010}}
          \end{tcolorbox}
\end{itemize}

\subsection{Reading Notes...}

...of \hyperlink{sr-original-paper}{the Paper}

\subsubsection{\hfil \S1. Definition of Simultaneity \hfil}

\begin{Definition}{Simultaneity}{def:simultaneity}
    If an event occurs at $(t, x, y, z)$, then all observers would see this event at $(t, x', y', z')$, where
    $x \ne x'$, $y \ne y'$, and $y \ne y'$
\end{Definition}

This definition is ideal but proves to be inefficient if we are going to look at a series of events happening one after
another, according to Einstein, because light takes time to travel. But two clocks can \textit{synchronize} in the
following way:

Suppose an event occurs at $A$ and a ray of light leaves from $A$ toward $B$ at $t_A$; the light is reflected from $B$
towards $A$ at $t_B$, and arrives back at $A$ at $t'_A$. The two clocks at $A$ and $B$ satisfies

\[
    t_B - t_A = t'_A - t_B
\]

which means

\begin{equation}\label{eq:tb-by-ta}
    t_B = \frac{t'_A + t_A}{2}
\end{equation}

Imagine a person holding a watch and manages to precisely record $t_A$ and $t'_A$, they will be able to state with
perfect confidence that any other person (or observer) at arbitrary location $B$ sees their event at time $t_B$, which
can be calculated by Eq.\ref{eq:tb-by-ta}, where both $t_A$ and $t'_A$ can be read at that person's hand watch

\begin{Definition}{Synchronism}{synchronism}
    Suppose a ray of light leaves from $A$ toward $B$ at ``A-time" at $t_A$, is reflected from $B$ toward $A$ at
    ``B-time" $t_B$, and arrives back at $A$ at ``A-time" $t'_A$. The two clocks are \textit{synchronous} by defintion
    if

    \begin{equation}\label{eq:synchronism}
        t_B - t_A = t'_A - t_B
    \end{equation}
\end{Definition}

It follows naturally that

\begin{enumerate}
    \item if the clock in $B$ is synchronous with the clock in $A$, then the clock in $A$ is synchronous with the clock
          in $B$, and
    \item if the clock in $A$ is synchronous with the clock in $B$ as well as with the clock in $C$, then the clocks in
          $B$ and $C$ are also synchronous relative to each other
\end{enumerate}

The \textbf{speed of light as a universal constant in empty space is thus}:

\begin{tcolorbox}[enhanced,colframe=green!75!black,colback=green!5!white]
    \begin{equation}
        V = \frac{2\overline{AB}}{t'_A - t_A}
    \end{equation}
\end{tcolorbox}

\begin{marker}
    and there is a BIG assumption: all clocks are at \textit{rest} in a system at \textit{rest}
\end{marker}

With that, we have a pretty good mechanism to talk about series of events in a system happening at different time,
because we know how to synchronize them \includegraphics[height=0.05\textwidth]{可莉-35.png}

\subsubsection{\hfil \S2. Something Stops Working\ldots \hfil}

\begin{tcolorbox}[
    enhanced,
    title=Principle of Relativity,
    colframe=green!50!black,
    colback=green!10!white,
    arc=0mm,
    colbacktitle=red!10!white,
    coltitle=green!50!black,
    attach boxed title to top text right={yshift=-0.50mm},
    boxed title style={
        skin=enhancedfirst jigsaw,
        size=small,arc=1mm,bottom=-1mm,
        interior style={fill=none, top color=green!30!white, bottom color=green!20!white}
    }
]
    The laws governing the changes of the state of any physical system do not depend on which one of two coordinate
    systems in uniform translational motion relative to each other these changes of the state are referred to
\end{tcolorbox}

\begin{tcolorbox}[
    enhanced,
    title=Principle of the Constancy of the Velocity of Light,
    colframe=green!50!black,
    colback=green!10!white,
    arc=0mm,
    colbacktitle=red!10!white,
    coltitle=green!50!black,
    attach boxed title to top text left={yshift=-0.50mm},
    boxed title style={
        skin=enhancedfirst jigsaw,
        size=small,arc=1mm,bottom=-1mm,
        interior style={fill=none, top color=green!30!white, bottom color=green!20!white}
    }
]
    \phantomsection\hypertarget{constant-c-principle}
    Each ray of light moves in the coordinate system ``at rest" with the definite velocity $V$ independent of
    whether this ray of light is emitted by a body at rest or a body in motion
\end{tcolorbox}

2 principles above along with the Def.\ref{def:synchronism} shall present us an surprising result that a moving rod
with a stationary length $r_{AB}$ will be measured to have a length different from $r_{AB}$, where both two lengths
are measured by a stationary observer in a system ``at rest"

\begin{tcolorbox}[
    breakable,
    parbox=false,
    skin=bicolor,
    sidebyside,
    boxrule=0pt,
    frame style={
        top color=blue!50!white
    },
    colback=red!5!white,
    colbacklower=green!5!white,
    title={Suppose the rod is moving in the x-direction at a constant speed $v$. Let the length of the moving rod, measured in the system at rest, be denoted as $r_{AB}$, where $A$ and $B$ are the two ends of the rod.}
]
    \begin{center}
        \textbf{Classically}
    \end{center}

    The observer measures the stationary rod to have a length of $t_B - t_A = t'_A - t_B = \frac{l}{V}$, where $V$ is
    the speed of light and $t_B, t_A, t'_A, t_B$ are all drawn from Def.\ref{def:synchronism}.

    The observer then measures the moving rod. The light travels, due to
    \href{https://en.wikipedia.org/wiki/Inertia}{Classical Intertia}, at a speed of $ \color{red} v + V$, relative to
    the observer. They see the light reaches the other end of the rod at $\Delta t = t_B - t_A$. The light ends up
    travelling a distance of $(v + V)\Delta t$; this distance must also equal to the sum of the \textit{stationary rod
    length} plus the \textit{extra distance that the rod travels}:

    \begin{equation}
        (v + V)\Delta t = r_{AB} + v\Delta t
    \end{equation}

    Thus

    \begin{equation}
        t_B - t_A = \frac{r_{AB}}{V}
    \end{equation}

    When the light reflects back:

    \begin{equation}
        (V - v)\Delta t = v\Delta t - r_{AB}
    \end{equation}

    \begin{equation}
        t'_A - t_B = \frac{r_{AB}}{V}
    \end{equation}

    Therefore, classically, a moving rod with a stationary length $r_{AB}$ is still measured the same length.

    \tcblower

    \begin{center}
        \textbf{Relativistically}
    \end{center}

    The observer measures the stationary rod to have a length of $t_B - t_A = t'_A - t_B = \frac{l}{V}$, where $V$ is
    the speed of light and $t_B, t_A, t'_A, t_B$ are all drawn from Def.\ref{def:synchronism}.

    The observer then measures the moving rod. The light travels, by
    \hyperlink{constant-c-principle}{the Principle of the Constant Speed of Light}, at a speed of $ \color{green} V$,
    relative to the observer. They see the light reaches the other end of the rod at $\Delta t = t_B - t_A$. The
    light ends up travelling a distance of $V\Delta t$; this distance must also equal to the sum of the
    \textit{stationary rod length} plus the \textit{extra distance that the rod travels}:

    \begin{equation}
        V\Delta t = r_{AB} + v\Delta t
    \end{equation}

    Thus

    \begin{equation}\label{eq:forward-relativistic-propagation}
        t_B - t_A = \frac{r_{AB}}{V - v}
    \end{equation}

    When the light reflects back:

    \begin{equation}
        V\Delta t = v\Delta t - r_{AB}
    \end{equation}

    \begin{equation}\label{eq:backward-relativistic-propagation}
        t'_A - t_B = \frac{r_{AB}}{V + v}
    \end{equation}

    Therefore, relativistically, a moving rod with a stationary length $r_{AB}$ is measured to be shorter.

\end{tcolorbox}

Note that neither Eq.\ref{eq:forward-relativistic-propagation} nor Eq.\ref{eq:backward-relativistic-propagation} is
related to the $\text{time} = \frac{\text{distance}}{\text{speed}}$. \href{https://qr.ae/psQeNd}{The numerators of
both equations are NOT the distance traveled and the denominator is not the speed of the travel}

\begin{tcolorbox}[
    enhanced,
    skin=enhancedlast jigsaw,interior hidden,
    boxsep=0pt,top=0pt,colframe=red,coltitle=red!50!black,
    attach boxed title to bottom center,
    boxed title style={empty,boxrule=0.5mm},
    varwidth boxed title=0.5\linewidth,
    underlay boxed title={
        \draw[white,line width=0.5mm]
        ([xshift=0.3mm-\tcboxedtitleheight*2,yshift=0.3mm]title.north west)
        --([xshift=-0.3mm+\tcboxedtitleheight*2,yshift=0.3mm]title.north east);
        \path[draw=red,top color=white,bottom color=red!50!white,line width=0.5mm]
        ([xshift=0.25mm-\tcboxedtitleheight*2,yshift=0.25mm]title.north west)
        cos +(\tcboxedtitleheight,-\tcboxedtitleheight/2)
        sin +(\tcboxedtitleheight,-\tcboxedtitleheight/2)
        -- ([xshift=0.25mm,yshift=0.25mm]title.south west)
        -- ([yshift=0.25mm]title.south east)
        cos +(\tcboxedtitleheight,\tcboxedtitleheight/2)
        sin +(\tcboxedtitleheight,\tcboxedtitleheight/2); },
    title=we have a contradiction,
    watermark graphics=烟绯-11.png,
    watermark opacity=0.3
]
    Clearly,

    \begin{equation}\label{eq:unsynchronized-clocks}
        \frac{r_{AB}}{V - v} \ne \frac{r_{AB}}{V + v}
    \end{equation}

    which means

    \[
        t_B - t_A \ne t'_A - t_B
    \]

    but if two clocks at $A$, $B$, and the clock at rest are all synchronized in accordance with Def.\ref{def:synchronism}, then \textit{Eq.\ref{eq:synchronism} must hold}, which also means

    \[
        t_B - t_A = t'_A - t_B
    \]

    in this case
\end{tcolorbox}

The observers co-moving with $A$ and $B$, according to Eq.\ref{eq:unsynchronized-clocks}, would thus find that the two
clocks \textbf{DO NOT} run synchronously while the observers in the system at rest syncing the two clocks using
Def.\ref{def:synchronism} would declare they \textbf{DO} run synchronously

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!55!black,
    title=How do we resolve the contradiction?
]
    \begin{center}
        \textbf{There is No \textit{Absolute} Simultaneity}
    \end{center}

    Instead, two events that are simultaneous when observed from some particular coordinate system can no longer be
    considered simultaneous when observed from a system that is moving relative to that system
\end{tcolorbox}

\subsubsection{\hfil \S3. Lorentz Transformation - Quantifying Non-Simultaneity \hfil}

Let's formulate the problem we see at the end of the previous section:

For every event at $(x, y, z, t)$ in a system $K$ at rest, there corresponds to an event $(\xi, \eta, \zeta, \tau)$ in
a system $k$ moving in the x-direction relative to $K$ that fixes the deviation from that event from K

This section derives the relation of $(\xi, \eta, \zeta, \tau)$ = $f(x, y, z, t)$

In this setup, after some time $t$, a point $x$ in $K$ shall become the point $x'$ in $k$ where

\[
    x' = x + vt
\]

Let's suppose that at time $\tau_0$ a light ray is sent from the origin of the system $k$ along the x-axis to $x'$ and
is reflected from there at time $\tau_1$ toward the origin, where it arrives at time $\tau_2$; we then must have

\[
    \tau_2 - \tau_1 = \tau_1 - \tau_0
\]

\begin{equation} \label{eq:tau_012}
    \frac{1}{2}(\tau_0 + \tau_2) = \tau_1
\end{equation}

Since it is also true that $\tau_2 = \tau_0 + \frac{x'}{V - v} + \frac{x'}{V + v}$, by writing out the parameters of
$\tau$ in Eq.\ref{eq:tau_012}, we have the following general equation of

\begin{equation}
    \frac{1}{2}\left[ \tau(0, 0, 0, t) + \tau\left( 0, 0, 0, \tau_0 + \frac{x'}{V - v} + \frac{x'}{V + v} \right) \right] = \tau\left( x', 0, 0, t + \frac{x'}{V -v} \right)
\end{equation}

Taking the derivative of $\tau$ with respect to $x'$ and applying the Chain Rule of

\[
    \frac{\partial\tau}{\partial x'} = \frac{\partial\tau}{\partial t}\frac{\partial t}{\partial x'}
\]

give us

\begin{equation}
    \frac{1}{2}\frac{\partial\tau}{\partial t}\left[ \frac{1}{V - v} + \frac{1}{V + v} \right] = \frac{\partial\tau}{\partial x'} + \left( \frac{1}{V - v} \right)\frac{\partial\tau}{\partial t}
\end{equation}

which simplifies down to

\begin{equation}\label{eq:tau_t_general}
    \tcbhighmath[
        enhanced,colframe=red,colback=white,arc=0pt,boxrule=1pt,
        fuzzy halo=1mm with blue!50!white,
        arc=2pt,
        boxrule=0pt,
        frame hidden
    ]{
        \frac{\partial\tau}{\partial x'} + \frac{v}{V^2 - v^2}\frac{\partial\tau}{\partial t} = 0
    }
\end{equation}

It should be noted that $\mathscr{L} = \frac{\partial}{\partial x'} + \frac{v}{V^2 - v^2}\frac{\partial}{\partial t}$
is a Linear operator
\footnote{See \href{https://trello.com/c/5L46ePJQ}{Partial Differential Equations}, Strauss, page 2 for definition of \textit{Linearity}}
because

\begin{align}
    \mathscr{L}(\tau_1 + \tau_2) &= \frac{\partial(\tau_1 + \tau_2)}{\partial x'} + \frac{v}{V^2 - v^2}\frac{\partial(\tau_1 + \tau_2)}{\partial t} \\
    &= \frac{\partial\tau_1}{\partial x'} + \frac{\partial\tau_2}{\partial x'} + \frac{v}{V^2 - v^2}\frac{\partial\tau_1}{\partial t} + \frac{v}{V^2 - v^2}\frac{\partial\tau_2}{\partial t} \\
    & = \mathscr{L}(\tau_1) + \mathscr{L}(\tau_2)
\end{align}

and

\begin{equation}
    \mathscr{L}(c\tau) = \frac{\partial c\tau}{\partial x'} + \frac{v}{V^2 - v^2}\frac{\partial c\tau}{\partial t} = c\left( \frac{\partial\tau}{\partial x'} + \frac{v}{V^2 - v^2}\frac{\partial\tau}{\partial t} \right) = c\mathscr{L}(\tau)
\end{equation}

Therefore Eq.\ref{eq:tau_t_general} has a solution of the form\footnote{See \href{https://trello.com/c/5L46ePJQ}{Partial Differential Equations}, Strauss, page 6}

\begin{equation}
    \tau = f\left( \frac{v}{V^2 - v^2} x - t \right)
\end{equation}

We shall declare that

\begin{equation}\label{eq:transformation}
    \tau = \underbrace{f\left( \frac{v}{V^2 - v^2} x - t \right)}_{Mathematics} = \underbrace{\varphi(v)\left( t - \frac{v}{V^2 - v^2} x \right)}_{Physics}
\end{equation}

which assumes that transformation between $\tau$ and $t$ must be \textit{linear} which can be inferred by the
following fundamental postulate\footnote{In a\href{https://download.wpsoftware.net/causality-lorentz-group-zeeman.pdf}{1964 paper}, Erik Christopher Zeeman showed that the Lorentz transformation can be proven by applying the homogeneity principle to equations that represent the corresponding quantities at rest. This shows that the Lorentz transformation must be linear in both space and time coordinates}:

\begin{tcolorbox}[
    enhanced,frame hidden,boxrule=0pt,interior style={top color=green!10!white,
    bottom color=green!10!white,middle color=green!50!yellow},
    fuzzy halo=1pt with green
]
    \begin{center}
        \textbf{Space and time are homogeneous}\footnote{\textit{What proof do we have that the universe is
        homogenous?} In physics we can't prove something like this. It must be a postulate - something we take as a
        fundamental assumption on which to base our theories. If the assumption is wrong then eventually we will find
        experimental evidence of this. What we can say is that currently there is no evidence for any lack of
        homegeneity or isotropy in the universe.}
    \end{center}
\end{tcolorbox}

With Eq.\ref{eq:transformation}, we shall deduct the first 3 positional transformations in $(\xi, \eta, \zeta, \tau)$.
For a light ray emitted at time $\tau = 0$ in the direction of increasing $\xi$, we have

\begin{equation}
    \xi = V\tau = V\varphi(v)\left( t - \frac{v}{V^2 - v^2} x \right)
\end{equation}

which, combined with Eq.\ref{eq:forward-relativistic-propagation}, gives

\begin{equation}\label{eq:x-transformation}
    \xi = \varphi(v)\left( \frac{V^2}{V^2 - v^2} \right) x
\end{equation}

Analogously, by considering light rays moving along the two other moving axes:

\begin{equation}\label{eq:y-transformation}
    \left\{
    \begin{array}{l}
        \eta = V\varphi(v)\left( t - \frac{v}{V^2 - v^2} x \right) \\
        y^2 + (vt)^2 = (Vt)^2 \\
        x = 0
    \end{array}
    \right.
    \Rightarrow
    \eta = \varphi(v)\frac{V}{\sqrt{V^2 - v^2}}y
\end{equation}

\begin{equation}\label{eq:z-transformation}
    \left\{
    \begin{array}{l}
        \zeta = V\varphi(v)\left( t - \frac{v}{V^2 - v^2} x \right) \\
        z^2 + (vt)^2 = (Vt)^2 \\
        x = 0
    \end{array}
    \right.
    \Rightarrow
    \zeta = \varphi(v)\frac{V}{\sqrt{V^2 - v^2}}z
\end{equation}

Note that Eq.\ref{eq:x-transformation}, \ref{eq:y-transformation}, and \ref{eq:z-transformation} are based on the
assumption that $k$ starts out at the same point of $K$'s origin

It does not lose generality to say if $k$ is moving relative to $K$ at speed $v$ then $K$ is moving relative to $k$ at
speed $-v$. Directly measuring coordinates in $K$ is effectively the same thing as transforming $(x, y, z, t)$ in $K$
to $(\xi, \eta, \zeta, \tau)$ and then back to $(x, y, z, t)$ so we have along the x-axis:

\begin{equation}
    x = \varphi(-v)\left( \frac{V^2}{V^2 - v^2} \right) \varphi(v)\left( \frac{V^2}{V^2 - v^2} \right) x = \varphi(v)\varphi(-v)\left( \frac{V^2}{V^2 - v^2} \right)^2 x
\end{equation}

where $v$ is the speed of $K$ relative to $K$, which is clearly 0. Therefore

\begin{equation}
    x = \varphi(v)\varphi(-v)\left( \frac{V^2}{V^2 - 0^2} \right)^2 x = \varphi(v)\varphi(-v) x
\end{equation}

which leads to the identity transformation of

\begin{equation}\label{eq:trans-coef-prod-1}
    \varphi(v)\varphi(-v) = 1
\end{equation}

Let's place a rod of length $l$ at the origin of $k$ perpendicular to $\xi$-axis. If $k$ is moving to the x-direction
relative to $K$ at speed $v$, the measurement of the rod length, which we denote $y_1$, in $K$ satisfy

\begin{equation}
    l = \varphi(v)\frac{V}{\sqrt{V^2 - v^2}}y_1
\end{equation}

By symmetry

\begin{equation}
    l = \varphi(-v)\frac{V}{\sqrt{V^2 - v^2}}y_1
\end{equation}

\begin{equation}\label{eq:trans-coef-equal}
    \varphi(v) = \varphi(-v)
\end{equation}

Combining \ref{eq:trans-coef-equal} and \ref{eq:trans-coef-prod-1} leads to

\begin{equation}
    \varphi(v) = 1
\end{equation}


\section{Fundamental principles of special relativity theory (SR)}

\subsection{On ``Principle of relativity (Galileo)"}

\subsubsection{Galilean invariance}

\href{https://en.wikipedia.org/wiki/Newton\%27s_laws_of_motion}{Newton's laws of motion} hold in all frames related
to one another by a \href{https://en.wikipedia.org/wiki/Galilean\_transformation}{Galilean transformation}. In
other words, all frames related to one another by such a transformation are inertial (meaning, Newton's equation of
motion is valid in these frames).\footnote{\href{https://en.wikipedia.org/wiki/Galilean_invariance}{Galilean invariance}}
The proof has been given by the book on page 2.

\section{Construction of the coordinates used by another observer}

\subsection{Why would the tangent of the angle is the speed in Fig. 1.2?}

Suppose $\mathcal{O}$ and $\mathcal{\bar{O}}$ both start out at the same position where $\mathcal{\bar{O}}$
moves along the $x$ at some speed. After $t_1$, observer $\mathcal{O}$ sees $\mathcal{\bar{O}}$ at position $x_1$:

\[ \mathcal{\bar{O}}_1 = (x_1, t_1) \]

Observer  $\mathcal{\bar{O}}$, however, still sees themself at $x = 0$:

\[ \mathcal{\bar{O}}_1 = (0, t_1) \]

By definition where ``$\bar{t}$ is the locus of events at constant $\bar{x} = 0$", $\bar{t}$ is the straight line
that passes the origin and the $(x_1, t_1)$:

\begin{tikzpicture}[scale=1.5]
    \draw[->] (-3,0) node[left] (w) {}--(5,0) node[right] (x) {x};
    \draw[->] (0,-3) node[below] (s) {}--(0,5) node[above] (t) {t};
    \draw[line width=.5pt] (.25,-.25) rectangle (-.25,.25) node (o) {};
    \draw[line width=.5] (o)--(135:1) node[above] () {$\mathcal{\bar{O}}_0$, $\mathcal{O}_0$};

    \draw[black,line width=2pt,->] (0,0)--(4,4) node[above] (t-bar) {{$\bar{t}$}};

    \draw[dashed,line width=.5] (3,0) node[below] (x1) {$x_1$} --(3,3);
    \draw[dashed,line width=.5] (0,3) node[left] (t1) {$t_1$} --(3,3);
    \draw[line width=.5pt] (2.75,3.25) rectangle (3.25,2.75) node (f) {};
    \draw (f) node[right] {$(x_1, t_1)$};

    \draw [blue,thick,domain=225:270] plot ({3 + cos(\x)}, {3 + sin(\x)}) node (speed) {};
    \draw (2.5, 1.8) node[right] (speed) {Tangent of this angle is $\bar{v}$};
\end{tikzpicture}

\section{Invariance of the interval}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    title={Why $(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2 - (\Delta t)^2 = 0$ for two events in the same light beam?}
    \phantomsection\hypertarget{zero-interval}
]

    Let's say, in a simplified 1D case, event $\mathcal{E} = (x_0, t_0)$ and $\mathcal{P} = (x_1, t_1)$.

    \[ (\Delta x)^2 - (\Delta t)^2 = (x_1 - x_0)^2 - (t_1 - t_0)^2 \]

    Since the speed of light is 1,

    \[ (x_1 - x_0)^2 - (t_1 - t_0)^2 = (x_1 - x_0)^2 - (t_1 \times 1 - t_0 \times 1)^2 = (x_1 - x_0)^2 - (x_1 - x_0)^2 = 0 \]
\end{tcolorbox}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    enhanced,
    attach boxed title to top center={yshift=-2mm},
    title={
        \parbox{10cm}{
            Why does the equation contains only $\boldsymbol{M}_{\alpha\beta} + \boldsymbol{M}_{\beta\alpha}$ terms when $\alpha \ne \beta$, which guarantees $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$?
        }
    }]

    \[
        \Delta\bar{s}^2 = \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right)
    \]
\end{tcolorbox}

Before spending too much time on expanding the equation, we can pick up a pair of indices of
$(\alpha, \beta) = (\alpha^*, \beta^*)$ where $\alpha^* \ne \beta^*$. Then we would definitely have the following 2
terms in the expansion:

\[ \boldsymbol{M}_{\alpha^*\beta^*}\left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right) \]
\[ \boldsymbol{M}_{\beta^*\alpha^*}\left(\Delta x^{\beta^*}\right)\left(\Delta x^{\alpha^*}\right) \]

Since

\[ \left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right) = \left(\Delta x^{\beta^*}\right)\left(\Delta x^{\alpha^*}\right) \]

We can then group these 2 terms and factor out the product, leaving

\[
    \left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right)\left( \boldsymbol{M}_{\alpha^*\beta^*} + \boldsymbol{M}_{\beta^*\alpha^*} \right)
\]

The terms of expanded $\Delta\bar{s}^2$ can be expressed in a matrix of

\[
    \renewcommand{\arraystretch}{4}
    \begin{bmatrix}
        \boldsymbol{M}_{00}\Delta x^0\Delta x^0 & \boldsymbol{M}_{01}\Delta x^0\Delta x^1 & \boldsymbol{M}_{02}\Delta x^0\Delta x^2  & \boldsymbol{M}_{03}\Delta x^0\Delta x^3 \\
        \boldsymbol{M}_{10}\Delta x^1\Delta x^0 & \boldsymbol{M}_{11}\Delta x^1\Delta x^1 & \boldsymbol{M}_{12}\Delta x^1\Delta x^2  & \boldsymbol{M}_{13}\Delta x^1\Delta x^3 \\
        \boldsymbol{M}_{20}\Delta x^2\Delta x^0 & \boldsymbol{M}_{21}\Delta x^2\Delta x^1 & \boldsymbol{M}_{22}\Delta x^2\Delta x^2  & \boldsymbol{M}_{23}\Delta x^2\Delta x^3 \\
        \boldsymbol{M}_{30}\Delta x^3\Delta x^0 & \boldsymbol{M}_{31}\Delta x^3\Delta x^1 & \boldsymbol{M}_{32}\Delta x^3\Delta x^2  & \boldsymbol{M}_{33}\Delta x^3\Delta x^3 \\
    \end{bmatrix}
\]

Because the off-diagonal terms always appear in pairs above, we could effectively replace them with their mean value:

\[
    \boldsymbol{M}_{\alpha^*\beta^*} = \boldsymbol{M}_{\beta^*\alpha^*} = \frac{\left( \boldsymbol{M}_{\alpha^*\beta^*} + \boldsymbol{M}_{\beta^*\alpha^*}\right)}{2}
\]

where $\alpha^* \ne \beta^*$. And since $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$ if $\alpha = \beta$, we conclude that

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black]
    \begin{center}
        $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$ for all $\alpha$ and $\beta$
    \end{center}
\end{tcolorbox}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    enhanced,
    attach boxed title to top center={yshift=-2mm},
    title={
        \parbox{10cm}{
            Why do we have the 2nd term in equation
        }
    }]

    \[
        \Delta\bar{s}^2 = \boldsymbol{M}_{00}\left(\Delta r\right)^2 + \tcbhighmath{2\left( \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta x^i \right)\Delta r} + \sum_{i = 1}^3\sum_{i = 1}^3 \boldsymbol{M}_{ij}\Delta x^i\Delta x^j
    \]
\end{tcolorbox}

\begin{align}
    \Delta\bar{s}^2 &= \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \sum_{\alpha = 0}^0\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) + \sum_{\alpha = 0}^3\sum_{\beta = 0}^0 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \sum_{\beta = 0}^3 \boldsymbol{M}_{0\beta}\Delta t \left(\Delta x^{\beta}\right) + \sum_{\alpha = 0}^3 \boldsymbol{M}_{\alpha0}\left(\Delta x^{\alpha}\right)\Delta t + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \boldsymbol{M}_{00}\left( \Delta t \right)^2 + \sum_{\beta = 1}^3 \boldsymbol{M}_{0\beta}\Delta t \left(\Delta x^{\beta}\right) + \sum_{\alpha = 1}^3 \boldsymbol{M}_{\alpha0}\left(\Delta x^{\alpha}\right)\Delta t + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \boldsymbol{M}_{00}\left( \Delta t \right)^2 + \boxed{2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x^i\right) \right]} + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \label{eq:expand}
\end{align}

\begin{tcolorbox}[
    breakable,
    parbox=false,
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    title={Why would $\boldsymbol{M}_{0i} = 0$ for $i = 1, 2, 3$ and $\boldsymbol{M}_{ij} = -\boldsymbol{M}_{00}\delta_{ij}$ in Equation \ref{eq:expand}?}]

    \begin{tcolorbox}[
        colframe=red,
        colback=red!10,
        coltitle=black,
        colbacktitle=red!20,
        title=The answer is: \textbf{not necessarily}. We are probably looking at a wrong problem.
        \phantomsection\hypertarget{wrong-postulate}
    ]
        The solution to exercise 1.8 takes $\Delta x_1 = -\Delta x_2$ to simplify the equation \ref{eq:details}. This is not sufficient, because what if $\Delta x_1 \ne -\Delta x_2$? This box takes a general approach where we \textbf{do not assume any relationship between $\Delta x_1$ and $\Delta x_2$}
    \end{tcolorbox}

    Note that this statement is based on the aforementioned assumption that $\Delta \bar{s}^2 = \Delta s^2 = 0$, which has been proved \hyperlink{zero-interval}{here}. Therefore, by \ref{eq:expand}, we have

    \begin{align}
        &\Delta\bar{s}^2(\Delta t, \Delta x_1) - \Delta\bar{s}^2(\Delta t, \Delta x_2) \\
        ={}& \boldsymbol{M}_{00}\left( \Delta t \right)^2 + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x^i\right) \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
        ={}& 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i\right) \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \nonumber\\
        & 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_2^i\right) \right] - \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) \\
        ={}& \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) + \nonumber\\
        & 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i\right) \right] - 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_2^i\right) \right] \\
        ={}& \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta} \left[ \left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) \right] + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i - \Delta x_2^i\right) \right] = 0 \label{eq:details}
    \end{align}

    We won't be able to go further unless with some assumed relationships between $\Delta x_1^i$ and $\Delta x_2^i$.
    But since \hyperlink{wrong-postulate}{we do not assume any relations between them}, let's step back and re-think about this problem then and forget about $\Delta x_1^i$ and $\Delta x_2^i$.

    We go through all these for the proof of invariance of the interval. This is to work out a relation between $\Delta s^2$ and $\Delta \bar{s}^2$. The \roundinlinebox[red]{detail} is about $\Delta x_1^i$ and $\Delta x_2^i$ but the \roundinlinebox[green]{goal} is to derive some form of

    \[
        \Delta \bar{s}^2 = f\left( \Delta s^2 \right) = \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right)
    \]

    where \[ \Delta s^2 = -(\Delta t)^2 + \sum_{i = 1}^3 (\Delta x^i)^2 \]

    \begin{center}
        \textit{Let's work on $f\left( \Delta s^2 \right)$ directly toward that goal then}
    \end{center}
\end{tcolorbox}

Assuming $\Delta \bar{s}^2 = \Delta s^2 = 0$, we have $\Delta t = \pm\Delta x$; plugging it into Eq. \ref{eq:expand} gives us

\begin{align}
    \Delta\bar{s}^2 &= \boldsymbol{M}_{00}\sum_{i = 1}^3\left( \Delta x^i \right)^2 + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\left( \Delta x^i \right)^2 \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \left( \boldsymbol{M}_{00} + 2\boldsymbol{M}_{0i} \right)\sum_{i = 1}^3\left( \Delta x^i \right)^2 + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \label{eq:suggestive}
\end{align}

\textit{Eq.\ref{eq:suggestive} seems to suggest a linear relationship between $\Delta s^2$ and $\Delta\bar{s}^2$}. How do we go about proving it? We now start the \href{https://en.wikipedia.org/wiki/Derivations\_of\_the\_Lorentz\_transformations#Rigorous\_Statement\_and\_Proof\_of\_Proportionality_of_ds2_and_ds\%E2\%80\%B22}{formal proof of \textbf{Invariance of Interval}}

\begin{mytheorem}{}{}
    Let $n, p \ge 1$ be integers, $d := n + p$ and $V$ a vector space over $\mathbb{R}$ of dimension $d$. Let $h$ be
    an indefinite-inner product\footnote{It's essentially a function which is the product of other two functions. For it's Mathematics, start with page 447 of \href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, 4th Edition} for everything you need to know about indefinite-inner product.} on $V$ with signature type $(n, p)$\footnote{Read Ch. 6 of \href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, 4th Edition} and then \href{https://mathworld.wolfram.com/MatrixSignature.html}{Matrix Signature}}. Suppose $g$ is a symmetric bilinear form on $V$
    such that the null set of the associated quadratic form of $h$ is contained in that of $g$ (i.e. suppose that for
    every $v \in V$, if $h(v, v) = 0$ then $g(v, v) = 0$). Then, there exists a constant $C \in \mathbb{R}$ such that
    $g = Ch$. Futhermore, if we assume $n \ne p$ and that $g$ also has signature type $(n, p)$, then we have $C > 0$
\end{mytheorem}

\begin{Definition}{Vector Space}{vector-space}
    The space $\mathbb{R}^n$ consists of all column vectors $\boldsymbol{v}$ with $n$ components\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 120}
\end{Definition}

\begin{Definition}{Linear Transformation}{linear-transformation}
    The transformation is \textbf{linear} if it meets these requirements for all $\boldsymbol{v}$ and $\boldsymbol{w}$\footnote{\href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, Strang, 4th Edition, 2009}, p. 375}:

    \begin{equation}
        T(c\boldsymbol{v} + d\boldsymbol{w}) = cT(\boldsymbol{v}) + dT(\boldsymbol{w})
    \end{equation}

    for all $c$ and $d$
\end{Definition}

\begin{Definition}{Field}{field}
    A field is a set of elements in which a pair of operations called multiplication and addition is defined analogous
    to the operations of multiplication and addition in the real number system (which is itself an example of a
    field)\footnote{\href{https://projecteuclid.org/ebooks/notre-dame-mathematical-lectures/Chapter-I-Linear-Algebra/chapter/Chapter-I-Linear-Algebra/ndml/1175197044}{A. Fields, Chapter I: Linear Algebra}, Galois Theory: Lectures Delivered at the University of Notre Dame, Project Euclid}
\end{Definition}

In each field F there exist unique elements called $0$ and $1$ which, under the operations of addition and
multiplication, behave with respect to all the other elements of F exactly as their correspondents in the real number
system. The analogy is, however, not complete:

\begin{itemize}
    \item Multiplication is not assumed to be commutative in every field, and
    \item a field may have only a finite number of elements
\end{itemize}

If multiplication in the field is commutative, then the field is called a \textbf{commutative field}.

How does ``Field" connect to ``Linear Algebra"?

\begin{itemize}
    \item It gives a more quantitative definition of Vector Space than Def.\ref{def:vector-space} does:
          \begin{Definition}{Vector Space (Field Theory)}{vector-space-field}
              If V is an additive abelian group\footnote{\href{https://en.wikipedia.org/wiki/Group\_\%28mathematics\%29\#Definition}{Definition on Group}} with elements $A, B, \ldots$, F a field with elements $a, b, \dots$, and if for each $a \in F$ and $A \in V$ the product $aA$ denotes element of V, then V is called a \textbf{(left) vector space over F} if the following assumptions hold:

              \begin{align}
                  a(A + B) &= aA + aB \\
                  (a + b)A &= aA + bA \\
                  a(bA) &= (ab)A \\
                  1A &= A \\
              \end{align}
          \end{Definition}
\end{itemize}
