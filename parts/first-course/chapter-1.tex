\section{Einstein's Original Paper on ``Special Relativity"}

\begin{marker}
    This section is not from the book, but simply my extra interests on the history of SR
\end{marker}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!85!black
]
    \phantomsection\hypertarget{sr-original-paper}
    The original paper is in \href{https://trello.com/c/ll9Yzd8h}{The Collected Papers of Albert Einstein, Vol.2}, page 140, \textit{On the Electrodynamics of Moving Bodies}
\end{tcolorbox}

Reading the original paper requires the prerequisites of

\begin{itemize}
    \item \href{https://en.wikipedia.org/wiki/Michelson\%E2\%80\%93Morley\_experiment}{Michelson-Morley Experiment}

          \begin{itemize}
              \item \href{https://youtu.be/3G_Q6AggQF8?si=ihpGM23NGq2PfsE2}{An excellent experiment intro}
              \item What I care most about this experiment is the \textbf{way we handle "unsolvable" problems}. Michelson-Morley
              experiment had led to extensive followups trying to explain what was seen in the experiment. All the
              \textit{mediocre} conclusion simply said: "Dude, we don't know." Albert Einstein innovated a new era of Physics out of this conflict. \textbf{When a problem seems to lead to a dead end, it's time to innovate; it's time to take on the risk and bring the human into a new world of new opportunities!}
          \end{itemize}

    \item \href{https://trello.com/c/SlPIXwCY}{Maxwell's Electrodynamics}

          \begin{tcolorbox}[
              colbacktitle=red!10!white,
              colback=blue!10!white,coltitle=red!70!black,
              title=Does the Electromagnetic Field \textit{physically} exist?
          ]
              ``There exists a model of the universe which includes a field known as the Electromagnetic Field. This
              model does a remarkably good job of predicting the observations we make in the world. It does so good at
              making such predictions that it is often phrased as 'existing in the
              world"\footnote{\href{https://philosophy.stackexchange.com/a/28010}{https://philosophy.stackexchange.com/a/28010}}
          \end{tcolorbox}
\end{itemize}

\subsection{Reading Notes...}

...of \hyperlink{sr-original-paper}{the Paper}

\subsubsection{\hfil \S1. Definition of Simultaneity \hfil}

\begin{Definition}{Simultaneity}{def:simultaneity}
    If an event occurs at $(t, x, y, z)$, then all observers would see this event at $(t, x', y', z')$, where
    $x \ne x'$, $y \ne y'$, and $y \ne y'$
\end{Definition}

This definition is ideal but proves to be inefficient if we are going to look at a series of events happening one after
another, according to Einstein, because light takes time to travel. But two clocks can \textit{synchronize} in the
following way:

Suppose an event occurs at $A$ and a ray of light leaves from $A$ toward $B$ at $t_A$; the light is reflected from $B$
towards $A$ at $t_B$, and arrives back at $A$ at $t'_A$. The two clocks at $A$ and $B$ satisfies

\[
    t_B - t_A = t'_A - t_B
\]

which means

\begin{equation}\label{eq:tb-by-ta}
    t_B = \frac{t'_A + t_A}{2}
\end{equation}

Imagine a person holding a watch and manages to precisely record $t_A$ and $t'_A$, they will be able to state with
perfect confidence that any other person (or observer) at arbitrary location $B$ sees their event at time $t_B$, which
can be calculated by Eq.\ref{eq:tb-by-ta}, where both $t_A$ and $t'_A$ can be read at that person's hand watch

\begin{Definition}{Synchronism}{synchronism}
    Suppose a ray of light leaves from $A$ toward $B$ at ``A-time" at $t_A$, is reflected from $B$ toward $A$ at
    ``B-time" $t_B$, and arrives back at $A$ at ``A-time" $t'_A$. The two clocks are \textit{synchronous} by defintion
    if

    \begin{equation}\label{eq:synchronism}
        t_B - t_A = t'_A - t_B
    \end{equation}
\end{Definition}

It follows naturally that

\begin{enumerate}
    \item if the clock in $B$ is synchronous with the clock in $A$, then the clock in $A$ is synchronous with the clock
          in $B$, and
    \item if the clock in $A$ is synchronous with the clock in $B$ as well as with the clock in $C$, then the clocks in
          $B$ and $C$ are also synchronous relative to each other
\end{enumerate}

The \textbf{speed of light as a universal constant in empty space is thus}:

\begin{tcolorbox}[enhanced,colframe=green!75!black,colback=green!5!white]
    \begin{equation}
        V = \frac{2\overline{AB}}{t'_A - t_A}
    \end{equation}
\end{tcolorbox}

\begin{marker}
    and there is a BIG assumption: all clocks are at \textit{rest} in a system at \textit{rest}
\end{marker}

With that, we have a pretty good mechanism to talk about series of events in a system happening at different time,
because we know how to synchronize them \includegraphics[height=0.05\textwidth]{可莉-35.png}

\subsubsection{\hfil \S2. Something Stops Working\ldots \hfil}

\begin{tcolorbox}[
    enhanced,
    title=Principle of Relativity,
    colframe=green!50!black,
    colback=green!10!white,
    arc=0mm,
    colbacktitle=red!10!white,
    coltitle=green!50!black,
    attach boxed title to top text right={yshift=-0.50mm},
    boxed title style={
        skin=enhancedfirst jigsaw,
        size=small,arc=1mm,bottom=-1mm,
        interior style={fill=none, top color=green!30!white, bottom color=green!20!white}
    }
]
    The laws governing the changes of the state of any physical system do not depend on which one of two coordinate
    systems in uniform translational motion relative to each other these changes of the state are referred to
\end{tcolorbox}

\begin{tcolorbox}[
    enhanced,
    title=Principle of the Constancy of the Velocity of Light,
    colframe=green!50!black,
    colback=green!10!white,
    arc=0mm,
    colbacktitle=red!10!white,
    coltitle=green!50!black,
    attach boxed title to top text left={yshift=-0.50mm},
    boxed title style={
        skin=enhancedfirst jigsaw,
        size=small,arc=1mm,bottom=-1mm,
        interior style={fill=none, top color=green!30!white, bottom color=green!20!white}
    }
]
    \phantomsection\hypertarget{constant-c-principle}
    Each ray of light moves in the coordinate system ``at rest" with the definite velocity $V$ independent of
    whether this ray of light is emitted by a body at rest or a body in motion
\end{tcolorbox}

2 principles above along with the Def.\ref{def:synchronism} shall present us an surprising result that a moving rod
with a stationary length $l$ will be measured to have a length different from $l$

Suppose the rod is moving in the x-direction at a constant speed $v$. Let the length of the moving rod, measured in the
system at rest, be denoted as $r_{AB}$, where $A$ and $B$ are the two ends of the rod.

If two clocks at $A$, $B$, and the clock at rest are all synchronized in accordance with Def.\ref{def:synchronism},
then \textit{Eq.\ref{eq:synchronism} must hold}

By \hyperlink{constant-c-principle}{the principle of the constant speed of light}, we can also safely say, without
worrying about which end of the rod corresponds to $A$ or $B$, that

\[
    t_B - t_A = \frac{r_{AB}}{V - v}
\]

and

\[
    t'_A - t_B = \frac{r_{AB}}{V + v}
\]

\begin{tcolorbox}[
    skin=enhancedlast jigsaw,interior hidden,
    boxsep=0pt,top=0pt,colframe=red,coltitle=red!50!black,
    attach boxed title to bottom center,
    boxed title style={empty,boxrule=0.5mm},
    varwidth boxed title=0.5\linewidth,
    underlay boxed title={
        \draw[white,line width=0.5mm]
        ([xshift=0.3mm-\tcboxedtitleheight*2,yshift=0.3mm]title.north west)
        --([xshift=-0.3mm+\tcboxedtitleheight*2,yshift=0.3mm]title.north east);
        \path[draw=red,top color=white,bottom color=red!50!white,line width=0.5mm]
        ([xshift=0.25mm-\tcboxedtitleheight*2,yshift=0.25mm]title.north west)
        cos +(\tcboxedtitleheight,-\tcboxedtitleheight/2)
        sin +(\tcboxedtitleheight,-\tcboxedtitleheight/2)
        -- ([xshift=0.25mm,yshift=0.25mm]title.south west)
        -- ([yshift=0.25mm]title.south east)
        cos +(\tcboxedtitleheight,\tcboxedtitleheight/2)
        sin +(\tcboxedtitleheight,\tcboxedtitleheight/2); },
    title=we have a contradiction
]
    Clearly,

    \begin{equation}\label{eq:unsynchronized-clocks}
        \frac{r_{AB}}{V - v} \ne \frac{r_{AB}}{V + v}
    \end{equation}

    which means

    \[
        t_B - t_A \ne t'_A - t_B
    \]

    but we have stated previously that \textit{Eq.\ref{eq:synchronism} must hold}, which also means

    \[
        t_B - t_A = t'_A - t_B
    \]

    in this case
\end{tcolorbox}

The observers co-moving with $A$ and $B$, according to Eq.\ref{eq:unsynchronized-clocks}, would thus find that the two
clocks \textbf{DO NOT} run synchronously while the observers in the system at rest syncing the two clocks using
Def.\ref{def:synchronism} would declare they \textbf{DO} run synchronously

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!55!black,
    title=How do we resolve the contradiction?
]
    \begin{center}
        \textbf{There is No \textit{Absolute} Simultaneity}
    \end{center}

    Instead, two events that are simultaneous when observed from some particular coordinate system can no longer be
    considered simultaneous when observed from a system that is moving relative to that system
\end{tcolorbox}

\subsubsection{\hfil \S3. Lorentz Transformation - Quantifying Non-Simultaneity \hfil}

\section{Fundamental principles of special relativity theory (SR)}

\subsection{On ``Principle of relativity (Galileo)"}

\subsubsection{Galilean invariance}

\href{https://en.wikipedia.org/wiki/Newton\%27s_laws_of_motion}{Newton's laws of motion} hold in all frames related
to one another by a \href{https://en.wikipedia.org/wiki/Galilean\_transformation}{Galilean transformation}. In
other words, all frames related to one another by such a transformation are inertial (meaning, Newton's equation of
motion is valid in these frames).\footnote{\href{https://en.wikipedia.org/wiki/Galilean_invariance}{Galilean invariance}}
The proof has been given by the book on page 2.

\section{Construction of the coordinates used by another observer}

\subsection{Why would the tangent of the angle is the speed in Fig. 1.2?}

Suppose $\mathcal{O}$ and $\mathcal{\bar{O}}$ both start out at the same position where $\mathcal{\bar{O}}$
moves along the $x$ at some speed. After $t_1$, observer $\mathcal{O}$ sees $\mathcal{\bar{O}}$ at position $x_1$:

\[ \mathcal{\bar{O}}_1 = (x_1, t_1) \]

Observer  $\mathcal{\bar{O}}$, however, still sees themself at $x = 0$:

\[ \mathcal{\bar{O}}_1 = (0, t_1) \]

By definition where ``$\bar{t}$ is the locus of events at constant $\bar{x} = 0$", $\bar{t}$ is the straight line
that passes the origin and the $(x_1, t_1)$:

\begin{tikzpicture}[scale=1.5]
    \draw[->] (-3,0) node[left] (w) {}--(5,0) node[right] (x) {x};
    \draw[->] (0,-3) node[below] (s) {}--(0,5) node[above] (t) {t};
    \draw[line width=.5pt] (.25,-.25) rectangle (-.25,.25) node (o) {};
    \draw[line width=.5] (o)--(135:1) node[above] () {$\mathcal{\bar{O}}_0$, $\mathcal{O}_0$};

    \draw[black,line width=2pt,->] (0,0)--(4,4) node[above] (t-bar) {{$\bar{t}$}};

    \draw[dashed,line width=.5] (3,0) node[below] (x1) {$x_1$} --(3,3);
    \draw[dashed,line width=.5] (0,3) node[left] (t1) {$t_1$} --(3,3);
    \draw[line width=.5pt] (2.75,3.25) rectangle (3.25,2.75) node (f) {};
    \draw (f) node[right] {$(x_1, t_1)$};

    \draw [blue,thick,domain=225:270] plot ({3 + cos(\x)}, {3 + sin(\x)}) node (speed) {};
    \draw (2.5, 1.8) node[right] (speed) {Tangent of this angle is $\bar{v}$};
\end{tikzpicture}

\section{Invariance of the interval}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    title={Why $(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2 - (\Delta t)^2 = 0$ for two events in the same light beam?}
    \phantomsection\hypertarget{zero-interval}
]

    Let's say, in a simplified 1D case, event $\mathcal{E} = (x_0, t_0)$ and $\mathcal{P} = (x_1, t_1)$.

    \[ (\Delta x)^2 - (\Delta t)^2 = (x_1 - x_0)^2 - (t_1 - t_0)^2 \]

    Since the speed of light is 1,

    \[ (x_1 - x_0)^2 - (t_1 - t_0)^2 = (x_1 - x_0)^2 - (t_1 \times 1 - t_0 \times 1)^2 = (x_1 - x_0)^2 - (x_1 - x_0)^2 = 0 \]
\end{tcolorbox}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    enhanced,
    attach boxed title to top center={yshift=-2mm},
    title={
        \parbox{10cm}{
            Why does the equation contains only $\boldsymbol{M}_{\alpha\beta} + \boldsymbol{M}_{\beta\alpha}$ terms when $\alpha \ne \beta$, which guarantees $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$?
        }
    }]

    \[
        \Delta\bar{s}^2 = \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right)
    \]
\end{tcolorbox}

Before spending too much time on expanding the equation, we can pick up a pair of indices of
$(\alpha, \beta) = (\alpha^*, \beta^*)$ where $\alpha^* \ne \beta^*$. Then we would definitely have the following 2
terms in the expansion:

\[ \boldsymbol{M}_{\alpha^*\beta^*}\left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right) \]
\[ \boldsymbol{M}_{\beta^*\alpha^*}\left(\Delta x^{\beta^*}\right)\left(\Delta x^{\alpha^*}\right) \]

Since

\[ \left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right) = \left(\Delta x^{\beta^*}\right)\left(\Delta x^{\alpha^*}\right) \]

We can then group these 2 terms and factor out the product, leaving

\[
    \left(\Delta x^{\alpha^*}\right)\left(\Delta x^{\beta^*}\right)\left( \boldsymbol{M}_{\alpha^*\beta^*} + \boldsymbol{M}_{\beta^*\alpha^*} \right)
\]

The terms of expanded $\Delta\bar{s}^2$ can be expressed in a matrix of

\[
    \renewcommand{\arraystretch}{4}
    \begin{bmatrix}
        \boldsymbol{M}_{00}\Delta x^0\Delta x^0 & \boldsymbol{M}_{01}\Delta x^0\Delta x^1 & \boldsymbol{M}_{02}\Delta x^0\Delta x^2  & \boldsymbol{M}_{03}\Delta x^0\Delta x^3 \\
        \boldsymbol{M}_{10}\Delta x^1\Delta x^0 & \boldsymbol{M}_{11}\Delta x^1\Delta x^1 & \boldsymbol{M}_{12}\Delta x^1\Delta x^2  & \boldsymbol{M}_{13}\Delta x^1\Delta x^3 \\
        \boldsymbol{M}_{20}\Delta x^2\Delta x^0 & \boldsymbol{M}_{21}\Delta x^2\Delta x^1 & \boldsymbol{M}_{22}\Delta x^2\Delta x^2  & \boldsymbol{M}_{23}\Delta x^2\Delta x^3 \\
        \boldsymbol{M}_{30}\Delta x^3\Delta x^0 & \boldsymbol{M}_{31}\Delta x^3\Delta x^1 & \boldsymbol{M}_{32}\Delta x^3\Delta x^2  & \boldsymbol{M}_{33}\Delta x^3\Delta x^3 \\
    \end{bmatrix}
\]

Because the off-diagonal terms always appear in pairs above, we could effectively replace them with their mean value:

\[
    \boldsymbol{M}_{\alpha^*\beta^*} = \boldsymbol{M}_{\beta^*\alpha^*} = \frac{\left( \boldsymbol{M}_{\alpha^*\beta^*} + \boldsymbol{M}_{\beta^*\alpha^*}\right)}{2}
\]

where $\alpha^* \ne \beta^*$. And since $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$ if $\alpha = \beta$, we conclude that

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black]
    \begin{center}
        $\boldsymbol{M}_{\alpha\beta} = \boldsymbol{M}_{\beta\alpha}$ for all $\alpha$ and $\beta$
    \end{center}
\end{tcolorbox}

\begin{tcolorbox}[
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    enhanced,
    attach boxed title to top center={yshift=-2mm},
    title={
        \parbox{10cm}{
            Why do we have the 2nd term in equation
        }
    }]

    \[
        \Delta\bar{s}^2 = \boldsymbol{M}_{00}\left(\Delta r\right)^2 + \tcbhighmath{2\left( \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta x^i \right)\Delta r} + \sum_{i = 1}^3\sum_{i = 1}^3 \boldsymbol{M}_{ij}\Delta x^i\Delta x^j
    \]
\end{tcolorbox}

\begin{align}
    \Delta\bar{s}^2 &= \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \sum_{\alpha = 0}^0\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) + \sum_{\alpha = 0}^3\sum_{\beta = 0}^0 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \sum_{\beta = 0}^3 \boldsymbol{M}_{0\beta}\Delta t \left(\Delta x^{\beta}\right) + \sum_{\alpha = 0}^3 \boldsymbol{M}_{\alpha0}\left(\Delta x^{\alpha}\right)\Delta t + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \boldsymbol{M}_{00}\left( \Delta t \right)^2 + \sum_{\beta = 1}^3 \boldsymbol{M}_{0\beta}\Delta t \left(\Delta x^{\beta}\right) + \sum_{\alpha = 1}^3 \boldsymbol{M}_{\alpha0}\left(\Delta x^{\alpha}\right)\Delta t + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \boldsymbol{M}_{00}\left( \Delta t \right)^2 + \boxed{2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x^i\right) \right]} + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \label{eq:expand}
\end{align}

\begin{tcolorbox}[
    breakable,
    parbox=false,
    colback=green!5!white,
    colframe=green!75!black,
    colbacktitle=red!30!white,
    title={Why would $\boldsymbol{M}_{0i} = 0$ for $i = 1, 2, 3$ and $\boldsymbol{M}_{ij} = -\boldsymbol{M}_{00}\delta_{ij}$ in Equation \ref{eq:expand}?}]

    \begin{tcolorbox}[
        colframe=red,
        colback=red!10,
        coltitle=black,
        colbacktitle=red!20,
        title=The answer is: \textbf{not necessarily}. We are probably looking at a wrong problem.
        \phantomsection\hypertarget{wrong-postulate}
    ]
        The solution to exercise 1.8 takes $\Delta x_1 = -\Delta x_2$ to simplify the equation \ref{eq:details}. This is not sufficient, because what if $\Delta x_1 \ne -\Delta x_2$? This box takes a general approach where we \textbf{do not assume any relationship between $\Delta x_1$ and $\Delta x_2$}
    \end{tcolorbox}

    Note that this statement is based on the aforementioned assumption that $\Delta \bar{s}^2 = \Delta s^2 = 0$, which has been proved \hyperlink{zero-interval}{here}. Therefore, by \ref{eq:expand}, we have

    \begin{align}
        &\Delta\bar{s}^2(\Delta t, \Delta x_1) - \Delta\bar{s}^2(\Delta t, \Delta x_2) \\
        ={}& \boldsymbol{M}_{00}\left( \Delta t \right)^2 + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x^i\right) \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
        ={}& 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i\right) \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \nonumber\\
        & 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_2^i\right) \right] - \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) \\
        ={}& \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) + \nonumber\\
        & 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i\right) \right] - 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_2^i\right) \right] \\
        ={}& \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta} \left[ \left(\Delta x_1^{\alpha}\right)\left(\Delta x_1^{\beta}\right) - \left(\Delta x_2^{\alpha}\right)\left(\Delta x_2^{\beta}\right) \right] + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\Delta t \left(\Delta x_1^i - \Delta x_2^i\right) \right] = 0 \label{eq:details}
    \end{align}

    We won't be able to go further unless with some assumed relationships between $\Delta x_1^i$ and $\Delta x_2^i$.
    But since \hyperlink{wrong-postulate}{we do not assume any relations between them}, let's step back and re-think about this problem then and forget about $\Delta x_1^i$ and $\Delta x_2^i$.

    We go through all these for the proof of invariance of the interval. This is to work out a relation between $\Delta s^2$ and $\Delta \bar{s}^2$. The \roundinlinebox[red]{detail} is about $\Delta x_1^i$ and $\Delta x_2^i$ but the \roundinlinebox[green]{goal} is to derive some form of

    \[
        \Delta \bar{s}^2 = f\left( \Delta s^2 \right) = \sum_{\alpha = 0}^3\sum_{\beta = 0}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right)
    \]

    where \[ \Delta s^2 = -(\Delta t)^2 + \sum_{i = 1}^3 (\Delta x^i)^2 \]

    \begin{center}
        \textit{Let's work on $f\left( \Delta s^2 \right)$ directly toward that goal then}
    \end{center}
\end{tcolorbox}

Assuming $\Delta \bar{s}^2 = \Delta s^2 = 0$, we have $\Delta t = \pm\Delta x$; plugging it into Eq. \ref{eq:expand} gives us

\begin{align}
    \Delta\bar{s}^2 &= \boldsymbol{M}_{00}\sum_{i = 1}^3\left( \Delta x^i \right)^2 + 2\left[ \sum_{i = 1}^3\boldsymbol{M}_{0i}\left( \Delta x^i \right)^2 \right] + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \\
    &= \left( \boldsymbol{M}_{00} + 2\boldsymbol{M}_{0i} \right)\sum_{i = 1}^3\left( \Delta x^i \right)^2 + \sum_{\alpha = 1}^3\sum_{\beta = 1}^3 \boldsymbol{M}_{\alpha\beta}\left(\Delta x^{\alpha}\right)\left(\Delta x^{\beta}\right) \label{eq:suggestive}
\end{align}

\textit{Eq.\ref{eq:suggestive} seems to suggest a linear relationship between $\Delta s^2$ and $\Delta\bar{s}^2$}. How do we go about proving it? We now start the \href{https://en.wikipedia.org/wiki/Derivations\_of\_the\_Lorentz\_transformations#Rigorous\_Statement\_and\_Proof\_of\_Proportionality_of_ds2_and_ds\%E2\%80\%B22}{formal proof of \textbf{Invariance of Interval}}

\begin{mytheorem}{}{}
    Let $n, p \ge 1$ be integers, $d := n + p$ and $V$ a vector space over $\mathbb{R}$ of dimension $d$. Let $h$ be
    an indefinite-inner product\footnote{Start with page 447 of \href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, 4th Edition} for everything you need to know about indefinite-inner product} on $V$ with signature type $(n, p)$\footnote{Read Ch. 6 of \href{https://trello.com/c/qHJeDNkU}{Introduction to Linear Algebra, 4th Edition} and then \href{https://mathworld.wolfram.com/MatrixSignature.html}{Matrix Signature}}. Suppose $g$ is a symmetric bilinear form on $V$
    such that the null set of the associated quadratic form of $h$ is contained in that of $g$ (i.e. suppose that for
    every $v \in V$, if $h(v, v) = 0$ then $g(v, v) = 0$). Then, there exists a constant $C \in \mathbb{R}$ such that
    $g = Ch$. Futhermore, if we assume $n \ne p$ and that $g$ also has signature type $(n, p)$, then we have $C > 0$
\end{mytheorem}

