\section{Differential Calculus}

\begin{Theorem}{
    Fundamental Lemma\footnote{\href{https://trello.com/c/byu9Pyy8}{Calculus with Analytic Geometry by George F. Simmons}, p. 680}
    \phantomsection\hypertarget{fundamental-lemma}
}{fundamental-lemma}
    Suppose that a function $z = f(x, y)$ and its partial derivatives $f_x$ and $f_y$ are defined at a point
    $(x_0, y_0)$, and also through some neighborhood of this point. Suppose further that $f_x$ and $f_y$ are continuous
    at $(x_0, y_0)$. Then the increment $\Delta z$ can be expressed in the form of

    \begin{equation}
        \Delta z = f_x(x_0, y_0)\Delta x + f_y(x_0, y_0)\Delta y + \epsilon_1\Delta x + \epsilon_2\Delta y
    \end{equation}

    where $\epsilon_1$ and $\epsilon_2 \rightarrow 0$ as $\Delta x$ and $\Delta y \rightarrow 0$
\end{Theorem}

To prove this
statement\footnote{\href{https://trello.com/c/byu9Pyy8}{Calculus with Analytic Geometry by George F. Simmons}, p. 841},
we analyze the change $\Delta z$ in 2 steps as shown in Fig.~\ref{fig:proof-fundamental-lemma}:

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw [<->] (0,4) -- (0,0) -- (4,0);
        \node [below right] at (4,0) {$x$};
        \node [left] at (0,4) {$y$};

        \draw[fill] (1,1) circle [radius=0.025];
        \draw[fill] (3,1) circle [radius=0.025];
        \draw[fill] (3,3) circle [radius=0.025];

        \node [below] at (1,1) {$(x_0, y_0)$};
        \node [below] at (3,1) {$(x_0 + \Delta x, y_0)$};
        \node [above] at (3,3) {$(x_0 + \Delta x, y_0 + \Delta y)$};

        \draw [->][thick, red] (1,1) -- node[above] {$\Delta x$} (3,1);
        \draw [->][thick, red] (3,1) -- node[left] {$\Delta y$} (3,3);
    \end{tikzpicture}
    \caption{We assume $\Delta z = f(x_0 + \Delta x, y_0 + \Delta y) - f(x_0, y_0)$ and $\Delta z = \Delta_1 z + \Delta_2 z$}
    \label{fig:proof-fundamental-lemma}
\end{figure}

\begin{enumerate}
    \item changing $x$ alone and moving from $(x_0, y_0)$ to $(x_0 + \Delta x, y_0)$, and then
    \item changing $y$ alone and moving from $(x_0 + \Delta x, y_0)$ to $(x_0 + \Delta x, y_0 + \Delta y)$
\end{enumerate}

We denote the first change in $z$ by $\Delta_1 z$, so that

\begin{equation}
    \Delta_1 z = f(x_0 + \Delta x, y_0) - f(x_0, y_0)
\end{equation}

By The Mean Value Theorem\footnote{
    \begin{Theorem}{
        The Mean Value Theorem\footnote{\href{https://trello.com/c/byu9Pyy8}{Calculus with Analytic Geometry by George F. Simmons}, p. 76}
    }{mean-value-theorem}
        Let $y = f(x)$ be a function with the following two properties:

        \begin{enumerate}
            \item $f(x)$ is continuous on the closed interval $[a, b]$; and
            \item $f(x)$ is differentiable on the open interval $(a, b)$
        \end{enumerate}

        Then there exists at least one point $c$ in the open interval $(a, b)$ such that

        \[
            f'(c) = \frac{f(b) - f(a)}{b - a}
        \]

        or equivalently,

        \[
            f(b) - f(a) = f'(c)(b - a)
        \]
    \end{Theorem}
}, we can write this as

\begin{equation}\label{eq:first-change-z-mean-val-theo}
\Delta_1 z = \Delta x f_x(x_1, y_0)
\end{equation}

where $x_1$ is between $x_0$ and $x_0 + \Delta x$. Smilary, if we denote the second part of the change in $z$ by
$\Delta_1 z$, so that

\begin{equation}
    \Delta_2 z = f(x_0 + \Delta x, y_0 + \Delta y) - f(x_0 + \Delta x, y_0)
\end{equation}

then

\begin{equation}\label{eq:second-change-z-mean-val-theo}
\Delta_2 z = \Delta y f_y(x_0 + \Delta x, y_1)
\end{equation}

where $y_1$ is between $y_0$ and $y_0 + \Delta y$.

Now as $\Delta x$ and $\Delta y \rightarrow 0$, $x_1 \rightarrow x_0$ and $y_1 \rightarrow y_0$. By the assumed
continuity of $f_x$ and $f_y$ at $(x_0, y_0)$, we can write

\begin{align}
    f_x(x_1, y_0) = f_x(x_0, y_0) + \epsilon_1 \label{eq:first-change-z-epsilon} \\
    f_y(x_0 + \Delta x, y_1) = f_y(x_0, y_0) + \epsilon_2 \label{eq:second-change-z-epsilon}
\end{align}

where $\epsilon_1$ and $\epsilon_2 \rightarrow 0$ as $\Delta x$ and $\Delta y \rightarrow 0$. Plugging
Eq.\ref{eq:first-change-z-epsilon} into Eq.\ref{eq:first-change-z-mean-val-theo} gives us

\begin{equation}
    \Delta_1 z = \Delta x\left[ f_x(x_0, y_0) + \epsilon_1 \right] = \Delta x f_x(x_0, y_0) + \Delta x\epsilon_1
\end{equation}

and similarly Eq.\ref{eq:second-change-z-epsilon} into Eq.\ref{eq:second-change-z-mean-val-theo}

\begin{equation}
    \Delta_2 z = \Delta y\left[ f_y(x_0, y_0) + \epsilon_2 \right] = \Delta y f_y(x_0, y_0) + \Delta y\epsilon_2
\end{equation}

Since we have assumed $\Delta z = \Delta_1 z + \Delta_2 z$

\begin{equation}
    \Delta z = \Delta x f_x(x_0, y_0) + \Delta x\epsilon_1 + \Delta y f_y(x_0, y_0) + \Delta y\epsilon_2 = f_x(x_0, y_0)\Delta x + f_y(x_0, y_0)\Delta y + \epsilon_1\Delta x + \epsilon_2\Delta y
\end{equation}

\qed

\footnote{\href{https://trello.com/c/byu9Pyy8}{Calculus with Analytic Geometry by George F. Simmons}, p. 681} Now Let
$f(x, y, z)$ be a function of 3 variables defined throughout some region of three-dimensional space, and let $P$ be a
point in this region. At what rate does $f$ change as we move away from $P$ in a specified direction? In the directions
of the positive x, y, and z-axes, we know that the rates of change off are given by the partial derivatives
$\frac{\partial f}{\partial x}$, $\frac{\partial f}{\partial y}$, and $\frac{\partial f}{\partial z}$. But how do we
calculate the rate of change of $f$ if we move away from $P$ in a direction that is not a coordinate direction?

Let $P = (x, y, z)$ and $\boldsymbol{R} = x\boldsymbol{i} + y\boldsymbol{j} + z\boldsymbol{k}$ being the position vector
of $P$. If we move away from $P$ to a nearby point $Q = (x + \Delta x, y + \Delta y, z + \Delta z)$, then the function
will change by an amoutn $\Delta f$. Let $\Delta s$ denote the distance between $P$ and $Q$, then we have

\begin{equation}\label{eq:def-df-over-ds}
    \frac{df}{ds} = \lim\limits_{\Delta s \rightarrow 0}\frac{\Delta f}{\Delta s}
\end{equation}

We further assume that $f(x, y, z)$ has continuous partial derivatives with respect to $x$, $y$, and $z$.

\begin{marker}
    Unless explicitly stated otherwise, all functions we deal with are always continuous in all of our discussions
\end{marker}

The \hyperlink{fundamental-lemma}{Fundamental Lemma} enables us to write $\Delta f$ in the form of

\begin{equation}\label{eq:df-in-3-partials}
    \Delta f = \frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y + \frac{\partial f}{\partial z}\Delta z + \epsilon_1\Delta x + \epsilon_2\Delta y + \epsilon_3\Delta z
\end{equation}

As $\Delta s \rightarrow 0$, i.e. as $\Delta x \rightarrow 0$, $\Delta y \rightarrow 0$, and $\Delta z \rightarrow 0$,
$\epsilon_1, \epsilon_2, \epsilon_3 \rightarrow 0$. Dividing Eq.\ref{eq:df-in-3-partials} by $\Delta s$ gives

\begin{equation}\label{eq:df-over-ds-in-3-partials}
    \lim\limits_{\Delta s \rightarrow 0}\frac{\Delta f}{\Delta s} = \frac{\partial f}{\partial x}\frac{dx}{ds} + \frac{\partial f}{\partial y}\frac{dy}{ds} + \frac{\partial f}{\partial z}\frac{dz}{ds}
\end{equation}

Combing Eq.\ref{eq:df-over-ds-in-3-partials} and \ref{eq:def-df-over-ds} results in

\begin{equation}\label{eq:partial-chain-rule}
    \tcbhighmath[
        enhanced,colframe=red,colback=white,arc=0pt,boxrule=1pt,
        fuzzy halo=1mm with blue!50!white,
        arc=2pt,
        boxrule=0pt,
        frame hidden
    ]{
        \frac{df}{ds} = \frac{\partial f}{\partial x}\frac{dx}{ds} + \frac{\partial f}{\partial y}\frac{dy}{ds} + \frac{\partial f}{\partial z}\frac{dz}{ds}
    }
\end{equation}

\subsection{Gradient}

\footnote{\href{https://trello.com/c/U6HhhDq6}{Introduction to Electrodynamics by Griffiths, 3rd}, p. 13} The theorem
on \hyperref[eq:partial-chain-rule]{partial derivaves states} that

\begin{equation}
    dT = \left( \frac{\partial T}{\partial x} \right) dx + \left( \frac{\partial T}{\partial y} \right) dy + \left( \frac{\partial T}{\partial z} \right) dz
\end{equation}

Writing it in the dot product form:

\begin{align}
    dT &= \left( \frac{\partial T}{\partial x}\boldsymbol{\hat{x}} + \frac{\partial T}{\partial y}\boldsymbol{\hat{y}} + \frac{\partial T}{\partial z}\boldsymbol{\hat{z}} \right) \cdot (dx\boldsymbol{\hat{x}} + dy\boldsymbol{\hat{y}} + dz\boldsymbol{\hat{z}}) \\
      &= \nabla T \cdot d\boldsymbol{l} \label{eq:dt-dot}
\end{align}

where

\begin{equation}\label{eq:def-gradient}
    \nabla T \equiv \frac{\partial T}{\partial x}\boldsymbol{\hat{x}} + \frac{\partial T}{\partial y}\boldsymbol{\hat{y}} + \frac{\partial T}{\partial z}\boldsymbol{\hat{z}}
\end{equation}

is the \textbf{gradient} of $T$. We also call $\nabla$ as the \textbf{vector operator} that \textit{acts upon} $T$

\begin{tcolorbox}[
    parbox=false,
    colbacktitle=red!10!white,
    colback=blue!10!white,coltitle=red!70!black,
    title=The Geometrical Interpretation of the Gradient
]
    The doc product~\ref{eq:dt-dot} can be written as

    \begin{equation}
        dT = \nabla T \cdot d\boldsymbol{l} = \vert \nabla T \vert \vert d\boldsymbol{l} \vert \cos{\theta}
    \end{equation}

    We soon realize that the \textit{maximum} change of T occurs when $\theta = 0$, therefore

    \begin{tcolorbox}[colback=red!10!white]
        \begin{center}
            \textcolor[HTML]{8F8FF5}{\textbf{The gradient $\nabla T$ points in the direction of the maximum increase of $T$,
            and its magnitude $\vert \nabla T \vert$ gives the slope (rate of increase) along this maximal direction}}
        \end{center}
    \end{tcolorbox}
\end{tcolorbox}

Now there are 3 ways the operator $\nabla$ can act:

\begin{enumerate}
    \item On a scalar function $T$: $\nabla T$ (the gradient, which we've discussed so far)
    \item On a vector function via the dot product: $\nabla \cdot \boldsymbol{v}$ (the \textbf{divergence})
    \item On a vector function via the cross product: $\nabla \times \boldsymbol{v}$ (the \textbf{curl})
\end{enumerate}

\subsection{Divergence}

From \hyperref[eq:def-gradient]{the definition of $\nabla$}, we construct the divergence

\begin{align}
    \nabla \cdot \boldsymbol{v} &= \left( \frac{\partial T}{\partial x}\boldsymbol{\hat{x}} + \frac{\partial T}{\partial y}\boldsymbol{\hat{y}} + \frac{\partial T}{\partial z}\boldsymbol{\hat{z}} \right) \cdot (v_x\boldsymbol{\hat{x}} + v_y\boldsymbol{\hat{y}} + v_z\boldsymbol{\hat{z}}) \label{eq:divergence-dot-product} \\
    &= \frac{\partial v_x}{\partial x} + \frac{\partial v_y}{\partial y} + \frac{\partial v_z}{\partial z}
\end{align}

%\begin{tcolorbox}[
%    parbox=false,
%    colbacktitle=red!10!white,
%    colback=blue!10!white,coltitle=red!70!black,
%    title=The Geometrical Interpretation of the Divergence
%]
%    The divergence is a measure of how much the vector $\boldsymbol{v}$ spreads out (diverges) from the point in question
%\end{tcolorbox}

\subsection{Curl}

\subsubsection{The Cross Prudct of Two Vectors}

\footnote{\href{https://trello.com/c/byu9Pyy8}{Calculus with Analytic Geometry by George F. Simmons}, p. 640} Many
problems in geometry require us to find a vector that is perpendicular to each of two given vectors $\boldsymbol{A}$ and
$\boldsymbol{B}$. A routine way of doing this is provided by the \textit{cross product} (or \textit{vector product}) of
$\boldsymbol{A}$ and $\boldsymbol{B}$, denoted by $\boldsymbol{A} \times \boldsymbol{B}$. This cross product is very
different from the \hyperref[eq:divergence-dot-product]{dot product} $\boldsymbol{A} \cdot \boldsymbol{B}$, because
$\boldsymbol{A} \times \boldsymbol{B}$ is a vector while $\boldsymbol{A} \cdot \boldsymbol{B}$ is a scalar.

Consider two nonzero vectors $\boldsymbol{A}$ and $\boldsymbol{B}$. Suppose their tails conincide and let $\theta$ be
the angle from $\boldsymbol{A}$ to $\boldsymbol{B}$ (\textit{not} from $\boldsymbol{B}$ to $\boldsymbol{A}$) with
$0 \le \theta \le \pi$.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw[-,fill=white!95!red](0,0)--(3,0)--(4,1)--(1,1)--cycle;
        \node at (2,0.5) {$|\textcolor{blue}{\boldsymbol{A}}\times \textcolor{red}{\boldsymbol{B}}|$};
        \draw[ultra thick,-latex,blue](0,0)--(3,0)node[midway,below]{$\boldsymbol{A}$};
        \draw[ultra thick,-latex,red](0,0)--(1,1)node[midway,above]{$\boldsymbol{B}$};
        \draw[ultra thick,-latex,blue!50!red](0,0)--(0,3)node[pos=0.7,right]{$\boldsymbol{A} \times \boldsymbol{B}$};
        \draw[ultra thick,-latex,blue!50!red](0,0)--(0,3)node[pos=0.5,right]{$\boldsymbol{n}$};
        \draw (0.6,0) arc [start angle=0,end angle=45,radius=0.6]
        node[pos=0.7,right]{$\theta$};
    \end{tikzpicture}
    \caption{The plane defined by $\boldsymbol{A}$ and $\boldsymbol{B}$}
    \label{fig:cross-prod}
\end{figure}

These 2 vectors determine a plane, as shown in Fig.~\ref{fig:cross-prod}. We now choose the unit
vector $\boldsymbol{n}$ which is normal (perpendicular) to this plane and whose direction is determined by the
\textit{right-hand thumb rule}\footnote{This means that if the right hand is placed so that the thumb is perpendicular
to the plane of $\boldsymbol{A}$ and $\boldsymbol{B}$ and the fingers curl from $\boldsymbol{A}$ to $\boldsymbol{B}$ in
the direction of angle $\theta$, then $\boldsymbol{n}$ points in the same direction as the thumb of this hand}.
\textit{This gives the direction of $\boldsymbol{A} \times \boldsymbol{B}$}

Vectors $\boldsymbol{A}$ and $\boldsymbol{B}$ also defines a parallelogram in this plane of area
$\vert \boldsymbol{A} \vert\vert \boldsymbol{B} \vert\sin{\theta}$, which defines \textit{the magnitude of
$\boldsymbol{A} \times \boldsymbol{B}$}.

\begin{Definition}{Cross Product of $\boldsymbol{A}$ and $\boldsymbol{B}$}{cross-product}
    \begin{equation}
        \boldsymbol{A} \times \boldsymbol{B} = \vert \boldsymbol{A} \vert\vert \boldsymbol{B} \vert\sin{\theta}
    \end{equation}
\end{Definition}

Our next objective is to develop a convenient formula for calculating $\boldsymbol{A} \times \boldsymbol{B}$ where

\begin{equation}\label{eq:cross-product-of-two}
    \boldsymbol{A} = a_1\boldsymbol{\hat{i}} + a_2\boldsymbol{\hat{j}} + a_3\boldsymbol{\hat{k}} \text{\ \ \ \ \ \ and \ \ \ \ \ \ }
    \boldsymbol{B} = b_1\boldsymbol{\hat{i}} + b_2\boldsymbol{\hat{j}} + b_3\boldsymbol{\hat{k}}
\end{equation}

We need to know that the cross product possesses the following algebraic properties

\begin{align}
    (c\boldsymbol{A}) \times \boldsymbol{B} &= c(\boldsymbol{A} \times \boldsymbol{B}) = \boldsymbol{A} \times (c\boldsymbol{B})\label{eq:cross-product-order}, \\
    \boldsymbol{A} \times (\boldsymbol{B} + \boldsymbol{C}) &= \boldsymbol{A} \times \boldsymbol{B} + \boldsymbol{A} \times \boldsymbol{C}\label{eq:cross-product-distributive-right}, \\
    (\boldsymbol{A} + \boldsymbol{B}) \times \boldsymbol{C} &= \boldsymbol{A} \times \boldsymbol{C} + \boldsymbol{B} \times \boldsymbol{C}\label{eq:cross-product-distributive-left}
\end{align}

Property~\ref{eq:cross-product-order} is easily established directly from Definition~\ref{def:cross-product}

We continue with our task of multiplying out the \hyperref[eq:cross-product-of-two]{cross product of the vectors} using
Eq.~\ref{eq:cross-product-order},~\ref{eq:cross-product-distributive-right}, and ~\ref{eq:cross-product-distributive-left}.
By substituting $b_1\boldsymbol{\hat{i}} + b_2\boldsymbol{\hat{j}} + b_3\boldsymbol{\hat{k}}$ with $\boldsymbol{B}$:

\begin{align}
    \boldsymbol{A} \times \boldsymbol{B} &= (a_1\boldsymbol{\hat{i}} + a_2\boldsymbol{\hat{j}} + a_3\boldsymbol{\hat{k}}) \times (b_1\boldsymbol{\hat{i}} + b_2\boldsymbol{\hat{j}} + b_3\boldsymbol{\hat{k}}) \\
    &= (a_1\boldsymbol{\hat{i}} + a_2\boldsymbol{\hat{j}} + a_3\boldsymbol{\hat{k}}) \times \boldsymbol{B} \\
    &= a_1\boldsymbol{\hat{i}} \times \boldsymbol{B} + a_2\boldsymbol{\hat{j}} \times \boldsymbol{B} + a_3\boldsymbol{\hat{k}} \times \boldsymbol{B} \\
    &= a_1\boldsymbol{\hat{i}} \times (b_1\boldsymbol{\hat{i}} + b_2\boldsymbol{\hat{j}} + b_3\boldsymbol{\hat{k}}) + a_2\boldsymbol{\hat{j}} \times (b_1\boldsymbol{\hat{i}} + b_2\boldsymbol{\hat{j}} + b_3\boldsymbol{\hat{k}}) + a_3\boldsymbol{\hat{k}} \times (b_1\boldsymbol{\hat{i}} + b_2\boldsymbol{\hat{j}} + b_3\boldsymbol{\hat{k}}) \\
    &= a_1 b_1 \boldsymbol{\hat{i}} \times \boldsymbol{\hat{i}} + a_1 b_2 \boldsymbol{\hat{i}} \times \boldsymbol{\hat{j}} + a_1 b_3 \boldsymbol{\hat{i}} \times \boldsymbol{\hat{k}} + a_2 b_1 \boldsymbol{\hat{j}} \times \boldsymbol{\hat{i}} + a_2 b_2 \boldsymbol{\hat{j}} \times \boldsymbol{\hat{j}} + a_2 b_3 \boldsymbol{\hat{j}} \times \boldsymbol{\hat{k}} + a_3 b_1 \boldsymbol{\hat{k}} \times \boldsymbol{\hat{i}} + a_3 b_2 \boldsymbol{\hat{k}} \times \boldsymbol{\hat{j}} + a_3 b_3 \boldsymbol{\hat{k}} \times \boldsymbol{\hat{k}}\label{eq:cross-product-complete-expand}
\end{align}

With the following corollary,

\begin{align}
    \boldsymbol{\hat{i}} \times \boldsymbol{\hat{i}} = 0 \\
    \boldsymbol{\hat{j}} \times \boldsymbol{\hat{j}} = 0 \\
    \boldsymbol{\hat{k}} \times \boldsymbol{\hat{k}} = 0 \\
    \boldsymbol{\hat{i}} \times \boldsymbol{\hat{j}} = -\boldsymbol{\hat{j}} \times \boldsymbol{\hat{i}} = \boldsymbol{\hat{k}} \\
    \boldsymbol{\hat{j}} \times \boldsymbol{\hat{k}} = -\boldsymbol{\hat{k}} \times \boldsymbol{\hat{j}} = \boldsymbol{\hat{i}} \\
    \boldsymbol{\hat{k}} \times \boldsymbol{\hat{i}} = -\boldsymbol{\hat{i}} \times \boldsymbol{\hat{k}} = \boldsymbol{\hat{j}} \\
\end{align}

Eq.~\ref{eq:cross-product-complete-expand} simplifies down to

\begin{align}
    \boldsymbol{A} \times \boldsymbol{B} &= a_1 b_2 \boldsymbol{\hat{k}} - a_1 b_3 \boldsymbol{\hat{j}} - a_2 b_1 \boldsymbol{\hat{k}} + a_2 b_3 \boldsymbol{\hat{i}} + a_3 b_1 \boldsymbol{\hat{j}} - a_3 b_2 \boldsymbol{\hat{i}} \\
    &= (a_2 b_3 - a_3 b_2)\boldsymbol{\hat{i}} - (a_1 b_3 - a_3 b_1)\boldsymbol{\hat{j}} + (a_1 b_2 - a_2 b_1)\boldsymbol{\hat{k}}\label{eq:cross-product-algebraic-form}
\end{align}

We recall that a determinant of order 2 is defined by

\begin{equation}
    \renewcommand{\arraystretch}{2}
    \begin{bmatrix}
        a_1 & a_2 \\
        b_1 & b_2 \\
    \end{bmatrix} = a_1 b_2 - a_2 b_1
\end{equation}

A determinant of order 3 can be defined in terms of determinants of order 2 as

\begin{equation}
    \begin{bmatrix}
        a_1 & a_2 & a_3 \\
        b_1 & b_2 & b_3 \\
        c_1 & c_2 & c_3 \\
    \end{bmatrix} =
    a_1 \begin{bmatrix}
        b_2 & b_3 \\
        c_2 & c_3 \\
    \end{bmatrix} -
    a_2 \begin{bmatrix}
        b_1 & b_3 \\
        c_1 & c_3 \\
    \end{bmatrix} +
    a_3 \begin{bmatrix}
        b_1 & b_2 \\
        c_1 & c_2 \\
    \end{bmatrix}
\end{equation}

Eq.~\ref{eq:cross-product-algebraic-form} is equivalent to

\begin{equation}
    \boldsymbol{A} \times \boldsymbol{B} =
    \begin{bmatrix}
        a_2 & a_3 \\
        b_2 & b_3 \\
    \end{bmatrix} \boldsymbol{\hat{i}} -
    \begin{bmatrix}
        a_1 & a_3 \\
        b_1 & b_3 \\
    \end{bmatrix} \boldsymbol{\hat{j}} +
    \begin{bmatrix}
        a_1 & a_2 \\
        b_1 & b_2 \\
    \end{bmatrix} \boldsymbol{\hat{k}} =
    \begin{bmatrix}
        \boldsymbol{\hat{i}} & \boldsymbol{\hat{j}} & \boldsymbol{\hat{j}} \\
        a_1 & a_2 & a_3 \\
        b_1 & b_2 & b_3 \\
    \end{bmatrix}
\end{equation}